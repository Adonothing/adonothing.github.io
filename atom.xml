<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adunas🍀の异世界</title>
  
  
  <link href="https://www.adunas.top/atom.xml" rel="self"/>
  
  <link href="https://www.adunas.top/"/>
  <updated>2024-03-04T00:55:25.000Z</updated>
  <id>https://www.adunas.top/</id>
  
  <author>
    <name>阿杜那斯🍀</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>离骚（节选）</title>
    <link href="https://www.adunas.top/posts/20240304a.html"/>
    <id>https://www.adunas.top/posts/20240304a.html</id>
    <published>2024-03-04T00:55:25.000Z</published>
    <updated>2024-03-04T00:55:25.000Z</updated>
    
    <content type="html"><![CDATA[<div class='poem'><div class='poem-title'>离骚（节选）</div><div class='poem-author'>屈原</div><p>长太息以掩涕兮，哀民生之多艰。余虽好修姱以鞿羁兮，謇朝谇而夕替。</p><p>既替余以蕙纕兮，又申之以揽茝。亦余心之所善兮，虽九死其犹未悔。</p><p>怨灵修之浩荡兮，终不察夫民心。众女嫉余之蛾眉兮，谣诼谓余以善淫。</p><p>固时俗之工巧兮，偭规矩而改错。背绳墨以追曲兮，竞周容以为度。</p><p>忳郁邑余侘傺兮，吾独穷困乎此时也。宁溘死以流亡兮，余不忍为此态也。</p><p>鸷鸟之不群兮，自前世而固然。何方圜之能周兮，夫孰异道而相安？</p><p>屈心而抑志兮，忍尤而攘诟。伏清白以死直兮，固前圣之所厚。</p><p>悔相道之不察兮，延伫乎吾将反。回朕车以复路兮，及行迷之未远。</p><p>步余马于兰皋兮，驰椒丘且焉止息。进不入以离尤兮，退将复修吾初服。</p><p>制芰荷以为衣兮，集芙蓉以为裳。不吾知其亦已兮，苟余情其信芳。</p><p>高余冠之岌岌兮，长余佩之陆离。芳与泽其杂糅兮，唯昭质其犹未亏。</p><p>忽反顾以游目兮，将往观乎四荒。佩缤纷其繁饰兮，芳菲菲其弥章。</p><p>民生各有所乐兮，余独好修以为常。虽体解吾犹未变兮，岂余心之可惩。</p></div><div class="audio"><audio controls preload><source src='https://picture.adunas.top/Audio/Read/LiSaoAdunasA.mp3' type='audio/mp3'>Your browser does not support the audio tag.</audio></div>]]></content>
    
    
    <summary type="html">🎗本文为高中所学离骚（节选）的内容</summary>
    
    
    
    <category term="阅读" scheme="https://www.adunas.top/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="阅读方法" scheme="https://www.adunas.top/tags/%E9%98%85%E8%AF%BB%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Latex</title>
    <link href="https://www.adunas.top/posts/20240225c.html"/>
    <id>https://www.adunas.top/posts/20240225c.html</id>
    <published>2024-02-25T03:38:02.000Z</published>
    <updated>2024-02-25T03:38:02.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="编程导航"><a href="#编程导航" class="headerlink" title="编程导航"></a><a href="./20240221b.html#Latex">编程导航</a></h1><h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><p>\documentclass</p>]]></content>
    
    
    <summary type="html">🍙本文记录Latex语法</summary>
    
    
    
    <category term="编程" scheme="https://www.adunas.top/categories/%E7%BC%96%E7%A8%8B/"/>
    
    
    <category term="Latex" scheme="https://www.adunas.top/tags/Latex/"/>
    
  </entry>
  
  <entry>
    <title>正则表达式</title>
    <link href="https://www.adunas.top/posts/20240225a.html"/>
    <id>https://www.adunas.top/posts/20240225a.html</id>
    <published>2024-02-25T01:43:26.000Z</published>
    <updated>2024-02-25T01:43:26.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="编程导航"><a href="#编程导航" class="headerlink" title="编程导航"></a><a href="./20240221b.html#正则表达式">编程导航</a></h1>]]></content>
    
    
    <summary type="html">🍤本文是正则表达式的教程</summary>
    
    
    
    <category term="编程" scheme="https://www.adunas.top/categories/%E7%BC%96%E7%A8%8B/"/>
    
    
    <category term="正则表达式" scheme="https://www.adunas.top/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>思考该不该吃完一颗难吃的苹果</title>
    <link href="https://www.adunas.top/posts/20240224c.html"/>
    <id>https://www.adunas.top/posts/20240224c.html</id>
    <published>2024-02-24T07:51:08.000Z</published>
    <updated>2024-02-24T07:51:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文学导航"><a href="#文学导航" class="headerlink" title="文学导航"></a><a href="./20240224d.html#思考该不该吃完一颗难吃的苹果">文学导航</a></h1><p>&emsp;&emsp;每次难吃的苹果我都咽下去了，但是这次我决定不吃了。（未完待续）</p>]]></content>
    
    
    <summary type="html">🍎每次难吃的苹果我都咽下去了，这次我决定不吃了</summary>
    
    
    
    <category term="文学" scheme="https://www.adunas.top/categories/%E6%96%87%E5%AD%A6/"/>
    
    
    <category term="思考" scheme="https://www.adunas.top/tags/%E6%80%9D%E8%80%83/"/>
    
  </entry>
  
  <entry>
    <title>文学导航</title>
    <link href="https://www.adunas.top/posts/20240224d.html"/>
    <id>https://www.adunas.top/posts/20240224d.html</id>
    <published>2024-02-24T07:42:23.000Z</published>
    <updated>2024-02-24T07:42:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文章导航总览"><a href="#文章导航总览" class="headerlink" title="文章导航总览"></a><a href="./20240221a.html#文学">文章导航总览</a></h1><h1 id="原创"><a href="#原创" class="headerlink" title="原创"></a>原创</h1><h2 id="思考该不该吃完一颗难吃的苹果"><a href="#思考该不该吃完一颗难吃的苹果" class="headerlink" title="思考该不该吃完一颗难吃的苹果"></a><a href="./20240224c.html">思考该不该吃完一颗难吃的苹果</a></h2><h1 id="授权发表"><a href="#授权发表" class="headerlink" title="授权发表"></a>授权发表</h1><h2 id="关于爱莉西亚局长的个人回忆"><a href="#关于爱莉西亚局长的个人回忆" class="headerlink" title="关于爱莉西亚局长的个人回忆"></a><a href="./20240222b.html">关于爱莉西亚局长的个人回忆</a></h2>]]></content>
    
    
    <summary type="html">🍛本文是文学分类的导航</summary>
    
    
    
    <category term="文章导航" scheme="https://www.adunas.top/categories/%E6%96%87%E7%AB%A0%E5%AF%BC%E8%88%AA/"/>
    
    
    <category term="文章导航" scheme="https://www.adunas.top/tags/%E6%96%87%E7%AB%A0%E5%AF%BC%E8%88%AA/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读</title>
    <link href="https://www.adunas.top/posts/20240224b.html"/>
    <id>https://www.adunas.top/posts/20240224b.html</id>
    <published>2024-02-23T17:17:02.000Z</published>
    <updated>2024-02-23T17:17:02.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文章导航总览"><a href="#文章导航总览" class="headerlink" title="文章导航总览"></a><a href="./20240221a.html#阅读">文章导航总览</a></h1><h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><h2 id="论文阅读方法"><a href="#论文阅读方法" class="headerlink" title="论文阅读方法"></a><a href="./20240224a.html">论文阅读方法</a></h2><h2 id="一种基于目测的未知目标运动分析方位角方法"><a href="#一种基于目测的未知目标运动分析方位角方法" class="headerlink" title="一种基于目测的未知目标运动分析方位角方法"></a><a href="./20240223a.html">一种基于目测的未知目标运动分析方位角方法</a></h2>]]></content>
    
    
    <summary type="html">🍜本文是阅读分类的导航</summary>
    
    
    
    <category term="文章导航" scheme="https://www.adunas.top/categories/%E6%96%87%E7%AB%A0%E5%AF%BC%E8%88%AA/"/>
    
    
    <category term="文章导航" scheme="https://www.adunas.top/tags/%E6%96%87%E7%AB%A0%E5%AF%BC%E8%88%AA/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读方法</title>
    <link href="https://www.adunas.top/posts/20240224a.html"/>
    <id>https://www.adunas.top/posts/20240224a.html</id>
    <published>2024-02-23T17:17:02.000Z</published>
    <updated>2024-02-23T17:17:02.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="阅读导航"><a href="#阅读导航" class="headerlink" title="阅读导航"></a><a href="./20240224b.html#论文阅读方法">阅读导航</a></h1><h1 id="资源下载"><a href="#资源下载" class="headerlink" title="资源下载"></a>资源下载</h1><h2 id="arXiv"><a href="#arXiv" class="headerlink" title="arXiv"></a>arXiv</h2><ol><li>官网：<a href="https://arxiv.org/">arXiv</a>。</li><li>视频介绍：<a href="https://www.bilibili.com/video/BV1pT4y1m7AF/?share_source=copy_web&amp;vd_source=6b55cb6788b1952e04c06b095d772810">【推倒论文付费墙，救人新冠疫情里，这是“反向知网”arXiv的30年】</a>。</li></ol><p>&emsp;&emsp;arXiv 是免费的、可供下载的论文库。是一个论文预印版网站。预印本（Preprint）是指科研工作者的研究成果还未在正式出版物上发表，而出于和同行交流目的自愿先在学术会议上或通过互联网发布的科研论文、科技报告等文章。而另一方面，arXiv有独特的作用：为了防止自己的 idea 在论文被收录前被别人剽窃，可以将预稿上传到 arXiv 作为预收录，因此这就是个可以证明论文原创性（上传时间戳）的文档收录网站。</p>]]></content>
    
    
    <summary type="html">🎗本文记录学习论文的资源和方法</summary>
    
    
    
    <category term="阅读" scheme="https://www.adunas.top/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="阅读方法" scheme="https://www.adunas.top/tags/%E9%98%85%E8%AF%BB%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：一种基于目测的未知目标运动分析方位角方法</title>
    <link href="https://www.adunas.top/posts/20240225b.html"/>
    <id>https://www.adunas.top/posts/20240225b.html</id>
    <published>2024-02-23T14:30:12.000Z</published>
    <updated>2024-02-23T14:30:12.000Z</updated>
    
    <content type="html"><![CDATA[<div class="note blue no-icon flat"><ol><li>b站视频：<a href="https://www.bilibili.com/video/BV1EC411z7Lz/?share_source=copy_web&amp;vd_source=6b55cb6788b1952e04c06b095d772810">【【IJRR最新成果】利用被忽视的视觉信息大幅提升目标定位可观性】</a></li><li>论文资源：<a href="https://arxiv.org/abs/2401.17117">A Bearing-Angle Approach for Unknown Target Motion Analysis Based on Visual Measurements</a></li></ol></div><h1 id="阅读导航"><a href="#阅读导航" class="headerlink" title="阅读导航"></a><a href="./20240224b.html#一种基于目测的未知目标运动分析方位角方法">阅读导航</a></h1><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This paper studies the problem of estimating the motion of a moving<br>target object using a moving monocular camera. The target’s geometric<br>information such as its physical size is <em>unknown</em> in advance. This<br>problem is important in many fields<br>[@Qiu2019; @Griffin2021; @Tekin2018]. Our present work is particularly<br>motivated by the task of aerial target pursuit, where a micro aerial<br>vehicle (MAV) uses its onboard camera to detect, localize, and then<br>pursue another flying MAV. The task of aerial target pursuit, originally<br>motivated by the interesting bird-catching-bird behaviors in nature<br>[@Brighton2019], potentially provides an effective approach to the<br>defense of misused MAV [@Rothe2019; @Dressel2019; @Vrba2020].</p><p>When a target has been detected in an image by a vision detection<br>algorithm, we usually obtain a <em>bounding box</em> that surrounds the<br>target’s image (see<br>Fig. <a href="#fig_architecture_outdoor">1</a>{reference-type=”ref”<br>reference=”fig_architecture_outdoor”}). The bounding box carries two<br>types of useful information that can be used to estimate the target’s<br>motion.</p><p>The first type of useful information is the <em>center point</em> of the<br>bounding box. The pixel coordinate of the center point can be used to<br>calculate the spatial <em>bearing vector</em> pointing from the camera to the<br>target based on the pin-hole camera model [@Ma2012]. Using the bearing<br>vector to estimate the target’s motion is referred to as <em>bearing-only</em><br>target motion estimation [@Fogel1988; @He2019; @Li2022]. As a problem<br>that has been studied for more than 40 years, bearing-only target motion<br>estimation was originally studied to estimate the motion of ships on the<br>ocean surface [@hoelzer1978modified], and regained increasing research<br>attention in recent years in vision-based target estimation tasks<br>[@Ponda2009; @Anjaly2018; @He2019].</p><p>Bearing-only target motion estimation requires an <em>observability<br>condition</em>: The observer must have higher-order motion than the target<br>and, more importantly, the higher-order motion must contain components<br>that are orthogonal to the target’s bearing vector [@Fogel1988].<br>Motivated by this observability condition, enormous works have studied<br>how an observer should move to enhance the observability<br>[@Hammel1989; @Sabet2016; @Anjaly2018; @He2019]. For instance, in our<br>recent work [@Li2022], we proposed a helical guidance law so that a MAV<br>moves along a helical curve to optimize the observability in the 3D<br>space.</p><p>A <em>limitation</em> of the observability condition of the classic<br>bearing-only approach is that the observer must move in the lateral<br>directions that are orthogonal to the bearing vector of the target. Such<br>additional lateral motion is usually unfavorable because it may conflict<br>with the desired motion of the observer in many tasks. For example, in<br>an aerial target pursuit task, the pursuer is desired to approach the<br>target as fast as possible and then keep stationary relative to the<br>target. Then, the additional lateral motion would conflict with the<br>desired motion. It is, therefore, important to study other ways that can<br>enhance the observability while avoiding unfavorable lateral motion.</p><p>The second type of useful information of a bounding box is its <em>size</em><br>(either width or height). The size of a bounding box is jointly<br>determined by several factors such as the target’s distance, the<br>target’s physical size, and the orientation of the camera. The target’s<br>physical size is usually unknown in many tasks, especially in those<br>antagonistic ones such as aerial pursuit of misused MAVs. As a result,<br>the size of the bounding box cannot directly infer the target’s<br>distance. Nevertheless, it carries valuable information for localizing<br>the target.</p><p>Surprisingly, the size information of the bounding box has not been well<br>explored so far. The work that is closely relevant to ours is the<br>state-of-the-art one in [@Griffin2021], where the size of a bounding box<br>is used to localize unknown target objects. Although the approach in<br>[@Griffin2021] is inspiring, it relies on two assumptions: The target<br>objects are stationary and the camera can only translate without<br>rotating. It is still an open problem how to estimate a target’s motion<br>when the two assumptions are not valid. Moreover, the theoretical role<br>of the size of a bounding box in target motion estimation has not been<br>fully understood so far. Although the work in [@Vrba2020] also utilizes<br>the size of the bounding box to estimate the target’s position, it is<br>assumed that the target’s physical size is known in advance.</p><p>Estimating the motion of moving objects is also a fundamental problem in<br>dynamic SLAM. For example, the works in [@Yang2019; @Qiu2019] firstly<br>estimate the camera’s pose and secondly estimate the target object’s<br>pose subject to a scale factor, and finally estimate the scale factor<br>from multi-view measurements. To estimate the target object’s pose<br>subject to a scale factor, [@Yang2019] and [@Qiu2019] rely on detecting,<br>respectively, a 3D bounding box and sufficient feature points inside the<br>2D bounding box. Different from [@Yang2019; @Qiu2019], our proposed<br>approach merely utilizes a 2D image bounding box without further<br>extracting feature points or a 3D bounding box inside the 2D bounding<br>box. As a result, one benefit is that this approach is more<br>computationally efficient. Moreover, this approach can handle the<br>challenging small-target case where the target object is far and hence<br>its image is small. In this case, it would be unreliable to extract<br>sufficient stable features or conduct 3D detection.</p><p>The aforementioned approaches in [@Griffin2021; @Yang2019; @Qiu2019] are<br>all based on multiple views. It is also possible to estimate the<br>target’s depth from a single view/image [@Tekin2018; @Vrba2020]. The<br>single-view approach however requires prior information of the objects.<br>Moreover, it would be unable to successfully localize target objects<br>with different sizes but similar appearances. In this paper, we focus on<br>the multi-view case.</p><p>In this paper, we propose a novel <em>bearing-angle</em> target motion<br>estimation approach that models a bounding box as bearing-angle<br>measurements. This approach can enhance the observability by fully<br>exploiting the information in a bounding box rather than relying on the<br>additional lateral motion of the observer. The benefit of the proposed<br>bearing-angle approach comes with no additional cost since the bounding<br>box is a standard output of object detection algorithms. The approach<br>simply exploits the angle information that has not been fully exploited<br>in the past. No additional sensing devices or special detection<br>algorithms are required.</p><p>The technical novelties of this approach are threefold.</p><p>1) The proposed approach does not directly use the size of a bounding<br>box because the size is variant to the orientation of the camera. That<br>is, even if the target’s relative position is unchanged, the size of the<br>bounding box still varies when the camera rotates. Motivated by this<br>problem, we convert the size of the bounding box to an angle subtended<br>by the target (see<br>Fig. <a href="#fig_architecture_outdoor">1</a>{reference-type=”ref”<br>reference=”fig_architecture_outdoor”}). The merit of using the angle<br>measurement is that it is <em>invariant</em> to the camera’s orientation change<br>(see Fig. <a href="#fig_cam_rotate">2</a>{reference-type=”ref”<br>reference=”fig_cam_rotate”}) and hence can greatly facilitate the<br>estimator design. In this way, the assumption in [@Griffin2021] that the<br>camera can only translate but not rotate can be avoided.</p><p>2) Although the bearing-angle approach incorporates an additional angle<br>measurement, it is nontrivial to see how to properly use this<br>measurement because the angle does not directly infer the target’s<br>distance given that the target’s size is unknown. We notice that the<br>angle is a joint nonlinear function of the target’s physical size and<br>relative distance. Hence, the state vector, which only consists of the<br>target’s position and velocity in the conventional bearing-only<br>approach, is augmented by the unknown target’s physical size. Since the<br>bearing and angle measurements are all nonlinear functions of the<br>target’s state, we establish a pseudo-linear Kalman filter to properly<br>utilize the measurements to enhance estimation stability. Both<br>simulation and real-world experiments verify the effectiveness of the<br>proposed estimator.</p><p>3) Although an additional angle measurement is used, an additional<br>unknown, the target’s physical size, is also introduced into the<br>estimator. It is, therefore, nontrivial to see how the additional angle<br>measurement can help improve the observability. Motivated by this<br>problem, we prove the necessary and sufficient observability condition<br>for bearing-angle target motion estimation. In particular, we show that<br>the target’s motion can be recovered if and only if the observer has a<br>higher-order motion than the target. Different from the bearing-only<br>case, the higher-order motion is <em>not</em> required to be in the lateral<br>directions that are orthogonal to the bearing vector. This is an<br>important enhancement of the observability. As we show in various<br>experiments, the bearing-angle approach can successfully recover the<br>target’s motion in many scenarios where the bearing-only approach fails.</p><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><h2 id="Algorithms-for-bearing-only-target-motion-estimation"><a href="#Algorithms-for-bearing-only-target-motion-estimation" class="headerlink" title="Algorithms for bearing-only target motion estimation"></a>Algorithms for bearing-only target motion estimation</h2><p>Bearing-only target motion analysis aims to estimate the target’s motion<br>states, such as position and velocity, using bearing measurement only.<br>It was originally motivated by ship localization and tracking in the<br>ocean [@hoelzer1978modified]. With the rapid development of small-scale<br>mobile robots equipped with cameras, the bearing-only approach regained<br>increasing attention in recent years [@Ponda2009; @Anjaly2018; @He2019].</p><p>Kalman filter-based estimators are widely used in the bearing-only<br>target motion. One challenge of applying the Kalman filter to the<br>bearing-only estimation is the nonlinearity of the bearing measurement.<br>The conventional extended Kalman filter (EKF) exhibits divergence<br>problems when applied to bearing-only target motion estimation<br>[@Aidala1979; @Lin2002]. Several methods have been proposed to solve<br>this problem. They can be divided into two types. The first type is the<br>modified polar EKF, which was first proposed in [@hoelzer1978modified].<br>In this approach, three observable quantities are separated from the<br>unobservable ones to prevent divergence. The work in [@Stallard1991]<br>extends this approach to the case of spherical coordinates to track<br>targets in 3D space. The second type is the pseudo-linear Kalman filter,<br>which is first proposed in [@Lingren1978] to solve the instability<br>problem by transforming the nonlinear measurement equation into a<br>pseudo-linear one. However, this transformation makes the noise become<br>non-Gaussian and highly correlated to the measurement matrix and then<br>causes estimation bias. Nevertheless, the work in [@Aidala1982]<br>theoretically proves that the velocity estimation has no bias, and the<br>position estimation bias can be removed by the observer’s maneuvers.</p><p>Recently, other estimation algorithms based on advanced but more complex<br>filters have been proposed. The work in [@Farina1999] uses the maximum<br>likelihood (MLE) algorithm to estimate the target’s motion using<br>bearing-only measurements. The comparison with the Cramer-Rao lower<br>bound indicates that the MLE-based estimator is effective against<br>measurement errors. The work in [@Dogancay2005] proposes a constrained<br>total least-squares algorithm, which can improve the estimation accuracy<br>when the error of bearing measurement is large. Three different<br>algorithms are used and compared in [@Lin2002]. The results show that<br>the EKF, the pseudo-linear filter, and the particle filter have similar<br>performances in most situations, while the EKF loses track when the<br>initial estimate error is large.</p><p>Another type of approach, called bearing-only trajectory triangulation<br>[@Avidan2000], estimates the target’s position from the perspective of<br>trajectory fitting. It reconstructs the trajectory by intersecting<br>parametric trajectory to a series of sight rays obtained from bearing<br>measurement. Once the trajectory is successfully fitted, the target’s<br>position at each time instant can be estimated by the intersection of<br>the bearing and the trajectory. The trajectory fitting relies on the<br>assumption of the trajectory’s shape. However, in many applications, the<br>target’s trajectory is complex and unknown in advance. Many consecutive<br>studies aim to relax this assumption in various ways based on<br>hypersurfaces [@Kaminski2004], parametric temporal polynomials<br>[@Yu2009], or compact basis vectors [@Park2015].</p><h2 id="Observability-analysis-of-bearing-only-target-motion-estimation"><a href="#Observability-analysis-of-bearing-only-target-motion-estimation" class="headerlink" title="Observability analysis of bearing-only target motion estimation"></a>Observability analysis of bearing-only target motion estimation</h2><p>Observability is a fundamental problem in bearing-only target motion<br>estimation. Early works mainly focus on whether the system is observable<br>or not. For example, the work in [@Lingren1978] uses the rank of<br>observation matrix to determine the observability. The work in<br>[@Fogel1988] extends the observability criterion in [@Nardone1981] to<br>the Nth-order target dynamics and inspires us for the observability<br>analysis in<br>Section <a href="#sec_observability_criteria">6</a>{reference-type=”ref”<br>reference=”sec_observability_criteria”}. All these conditions indicate<br>that the observer must have extra high-order motion in the lateral<br>direction. The observability condition can be significantly relaxed in<br>our approach.</p><p>Unlike the works on determining whether the system is observable or not,<br>some studies focus on quantifying the observability degree. The work in<br>[@Hammel1989] first introduces the Fisher information matrix (FIM) into<br>the observability analysis. The works in [@Sabet2016] and [@Anjaly2018]<br>use FIM-based objective functions to maximize observability. We also use<br>the FIM in our former work [@Li2022] to optimize the 3D helical guidance<br>law for better observability. Another method called the geometric method<br>uses the geometric relationship between the target and the observer in<br>two consecutive time instants to derive the measure of observability<br>[@He2019; @Woffinden2009], and the results are consistent with those<br>derived using FIM. Compared to the bearing-only approach, the<br>observability degree of our bearing-angle method is sufficient to<br>estimate the target’s motion in many common scenarios such as tracking<br>and guidance (see experiment results in<br>Figs. <a href="#fig_matlab_3">[fig_matlab_3]</a>{reference-type=”ref”<br>reference=”fig_matlab_3”}<br>and <a href="#fig_outdoor_1">[fig_outdoor_1]</a>{reference-type=”ref”<br>reference=”fig_outdoor_1”}).</p><h1 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h1><p><img src="fig_cam_rotate" alt="The size of the bounding box varies when the camera rotates. Bycontrast, the angle subtended by the target object is invariant to thecamera&#39;s orientation change."></p><p>Consider a target object moving in the 3D space. Its position and<br>velocity at time $t_k$ are denoted as $p_T(t_k) \in\mathbb{R}^3$ and<br>$v_T(t_k) \in\mathbb{R}^3$, respectively. Suppose there is an observer<br>carrying a monocular camera to observe the target. The position of the<br>observer is denoted as $p_o(t_k) \in\mathbb{R}^3$. Here, we assume that<br>the observer/camera’s pose including its position and orientation can be<br>obtained in other ways. For example, it can be measured directly by RTK<br>GPS [@Li2022] or estimated by visual inertial odometry [@Qiu2019]. In<br>the rest of the paper, the dependence of a variable on $t_k$ is dropped<br>when the context is clear.</p><p>If the target object can be detected by a vision algorithm, we can<br>obtain a bounding box surrounding the target object in the image. Two<br>types of information carried by the bounding box can be used to estimate<br>the motion of the target.</p><p>First, the center point of the bounding box can be used to calculate the<br><em>bearing</em> vector of the target. In particular, denote<br>${g} \in \mathbb{R}^3$ as the unit bearing vector pointing from $p<em>o$ to<br>$p_T$. Suppose ${P}</em>\text{cam}\in\mathbb{R}^{3\times3}$ is the intrinsic<br>parameter matrix of the camera [@Ma2012<br>Section <a href="#section_bearing-angle-target-motion-estimator">4</a>{reference-type=”ref”<br>reference=”section<em>bearing-angle-target-motion-estimator”}], and<br>${R}</em>\text{c}^\text{w} \in\mathbb{R}^{3\times 3}$ is the rotation from<br>the camera frame to the world frame. Then, the bearing vector $g$ can be<br>calculated as <script type="math/tex">\begin{aligned}{g} =\dfrac{{R}_\text{c}^\text{w}{P}_\text{cam}^{-1}{q}_{\rm pix}}{\|{R}_\text{c}^\text{w}{P}_\text{cam}^{-1}{q}_{\rm pix}\|},\end{aligned}</script> where<br>${q}<em>{\rm pix} =[x</em>{\rm pix} , y<em>{\rm pix} , 1]^\mathrm{T} \in \mathbb{R}^3$.<br>Here, $(x</em>{\rm pix} ,y_{\rm pix})$ is the pixel coordinate of the center<br>point of the bounding box.</p><p>Second, the size of the bounding box can be used to calculate the<br><em>angle</em> subtended by the target in the camera’s field of view. The<br>reason that we convert the bounding box’s size to the angle is that the<br>angle is invariant to the camera’s orientation change (see<br>Fig. <a href="#fig_cam_rotate">2</a>{reference-type=”ref”<br>reference=”fig<em>cam_rotate”}). In particular, let $s</em>{\rm pix}$ denote<br>the size of the bounding box. It can be either the width or the height.<br>Let $\theta \in (0,\pi/2)$ be the angle. According to the pin-hole<br>camera model [@Ma2012<br>Section <a href="#section_bearing-angle-target-motion-estimator">4</a>{reference-type=”ref”<br>reference=”section_bearing-angle-target-motion-estimator”}] and the law<br>of cosine (see Fig. <a href="#fig_cam_rotate">2</a>{reference-type=”ref”<br>reference=”fig_cam_rotate”}), the angle can be calculated as</p><p><script type="math/tex">\begin{aligned}\theta = \arccos\left(\dfrac{l_\mathrm{left}^2 + l_\mathrm{right}^2 - s_\mathrm{pix}^2}{2l_\mathrm{left}l_\mathrm{right}}\right),\end{aligned}</script> where<br>$l<em>\mathrm{left}=\sqrt{(f/\alpha)^2+(\delta x-s</em>\mathrm{pix}/2)^2+\delta y^2}\in\mathbb{R}$<br>and<br>$l<em>\mathrm{right}=\sqrt{(f/\alpha)^2+(\delta x+s</em>\mathrm{pix}/2)^2+\delta y^2}\in\mathbb{R}$<br>are the distances in pixel from the camera center to the middle points<br>of the left and right sides of the bounding box, respectively<br>(Fig. <a href="#fig_architecture_outdoor">1</a>{reference-type=”ref”<br>reference=”fig<em>architecture_outdoor”}). Moreover, $f$ and $\alpha$<br>denote the camera’s focal length and single pixel size, respectively.<br>$i</em>{\text{width}}$ and $i<em>{\text{height}}$ represent the width and the<br>height of the whole image in pixels, respectively.<br>$\delta x=|x</em>\text{pix}-i<em>\text{width}/2|\in\mathbb{R}$ and<br>$\delta y = |y</em>\text{pix}-i_\text{height}/2|\in\mathbb{R}$ are the<br>distances between the center of the bounding box and the center of the<br>image.</p><p>::: figure*<br><img src="fig_architecture_algorithm" alt="image">{width=”1\linewidth”}<br>:::</p><p>Our goal is to estimate the target’s position and velocity, $p_T$ and<br>$v_T$, based on the noisy measurements of the bearing vector ${g}$ and<br>the angle $\theta$ together with the observer’s own position $p_o$. To<br>achieve this goal, we propose a new bearing-angle target motion<br>estimator<br>(Fig. <a href="#fig_architecture_algorithm">[fig_architecture_algorithm]</a>{reference-type=”ref”<br>reference=”fig_architecture_algorithm”}). The estimator is introduced in<br>detail in<br>Section <a href="#section_bearing-angle-target-motion-estimator">4</a>{reference-type=”ref”<br>reference=”section_bearing-angle-target-motion-estimator”}. The<br>observability of this estimator is analyzed based on Kalman’s<br>observability criterion in<br>Section <a href="#sec_analysis_of_observability_matrix">5</a>{reference-type=”ref”<br>reference=”sec_analysis_of_observability_matrix”}. We further prove a<br>necessary and sufficient observability condition of the observer in<br>Section <a href="#sec_observability_criteria">6</a>{reference-type=”ref”<br>reference=”sec_observability_criteria”}. Numerical simulation results<br>are given in Section <a href="#sec_matlab_simulation">7</a>{reference-type=”ref”<br>reference=”sec_matlab_simulation”}. More realistic AirSim simulation<br>results are given in<br>Section <a href="#sec_airsim_simulation">8</a>{reference-type=”ref”<br>reference=”sec_airsim_simulation”}. Finally, real-world experiments are<br>given in<br>Section <a href="#sec_real_world_experimental_validation">9</a>{reference-type=”ref”<br>reference=”sec_real_world_experimental_validation”}.</p><h1 id="Bearing-Angle-Target-Motion-Estimator"><a href="#Bearing-Angle-Target-Motion-Estimator" class="headerlink" title="Bearing-Angle Target Motion Estimator"></a>Bearing-Angle Target Motion Estimator</h1>]]></content>
    
    
    <summary type="html">🧵本文研究了使用移动单目相机估计移动目标物体运动的问题</summary>
    
    
    
    <category term="阅读" scheme="https://www.adunas.top/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="视觉导航" scheme="https://www.adunas.top/tags/%E8%A7%86%E8%A7%89%E5%AF%BC%E8%88%AA/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：一种基于目测的未知目标运动分析方位角方法</title>
    <link href="https://www.adunas.top/posts/20240223a.html"/>
    <id>https://www.adunas.top/posts/20240223a.html</id>
    <published>2024-02-23T14:30:12.000Z</published>
    <updated>2024-02-23T14:30:12.000Z</updated>
    
    <content type="html"><![CDATA[<div class="note blue no-icon flat"><ol><li>b站视频：<a href="https://www.bilibili.com/video/BV1EC411z7Lz/?share_source=copy_web&amp;vd_source=6b55cb6788b1952e04c06b095d772810">【【IJRR最新成果】利用被忽视的视觉信息大幅提升目标定位可观性】</a></li><li>论文资源：<a href="https://arxiv.org/abs/2401.17117">A Bearing-Angle Approach for Unknown Target Motion Analysis Based on Visual Measurements</a></li></ol></div><h1 id="阅读导航"><a href="#阅读导航" class="headerlink" title="阅读导航"></a><a href="./20240224b.html#一种基于目测的未知目标运动分析方位角方法">阅读导航</a></h1><h1 id="A-Bearing-Angle-Approach-for-Unknown-Target-Motion-Analysis-Based-on-Visual-Measurements"><a href="#A-Bearing-Angle-Approach-for-Unknown-Target-Motion-Analysis-Based-on-Visual-Measurements" class="headerlink" title="A Bearing-Angle Approach for Unknown Target Motion Analysis Based on Visual Measurements"></a>A Bearing-Angle Approach for Unknown Target Motion Analysis Based on Visual Measurements</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>&emsp;&emsp;Vision-based estimation of the motion of a moving target is usually formulated as a <em>bearing-only</em> estimation problem where the visual measurement is modeled as a bearing vector. Although the bearing-only approach has been studied for decades, a <em>fundamental limitation</em> of this approach is that it requires extra lateral motion of the observer to enhance the target’s observability. Unfortunately, the extra lateral motion conflicts with the desired motion of the observer in many tasks.<br>It is well-known that, once a target has been detected in an image, a bounding box that surrounds the target can be obtained.<br>Surprisingly, this common visual measurement especially its size information has not been well explored up to now.<br>In this paper, we propose a new <em>bearing-angle</em> approach to estimate the motion of a target by modeling its image bounding box as bearing-angle measurements.<br>Both theoretical analysis and experimental results show that this approach can significantly enhance the observability <em>without</em> relying on additional lateral motion of the observer.<br>The benefit of the bearing-angle approach comes with no additional cost because a bounding box is a standard output of object detection algorithms.<br>The approach simply exploits the information that has not been fully exploited in the past.<br>No additional sensing devices or special detection algorithms are required.</p><h2 id="Keywords"><a href="#Keywords" class="headerlink" title="Keywords"></a>Keywords</h2><p>Bearing-only target motion estimation, Pseudo-linear Kalman filter, Observability enhancement</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>&emsp;&emsp;This paper studies the problem of estimating the motion of a moving target object using a moving monocular camera. The target’s geometric information such as its physical size is <em>unknown</em> in advance. This problem is important in many fields<!--  \citep{Qiu2019, Griffin2021, Tekin2018} -->.<br>Our present work is particularly motivated by the task of aerial target pursuit, where a micro aerial vehicle (MAV) uses its onboard camera to detect, localize, and then pursue another flying MAV.<br>The task of aerial target pursuit, originally motivated by the interesting bird-catching-bird behaviors in nature<!--  \citep{Brighton2019} -->, potentially provides an effective approach to the defense of misused MAV<!--  \citep{Rothe2019, Dressel2019, Vrba2020} -->.</p><p><a id= "fig_architecture_outdoor"></a><br><img src="https://picture.adunas.top/Article/arXiv-2401.17117v1/fig_architecture_outdoor.png" alt="Fig.1 An observer MAV observes a target MAV with a monocular camera. The bearing $g$ and angle $\theta$ can be obtained from the bounding box that surrounds the target in the image."></p><p>&emsp;&emsp;When a target has been detected in an image by a vision detection algorithm, we usually obtain a <em>bounding box</em> that surrounds the target’s image (see <a href="#fig_architecture_outdoor">Fig.1</a>).<br>The bounding box carries two types of useful information that can be used to estimate the target’s motion.</p><p>&emsp;&emsp;The first type of useful information is the <em>center point</em> of the bounding box.<br>The pixel coordinate of the center point can be used to calculate the spatial <em>bearing vector</em> pointing from the camera to the target based on the pin-hole camera model<!--  \citep{Ma2012} -->.<br>Using the bearing vector to estimate the target’s motion is referred to as <em>bearing-only</em> target motion estimation<!--  \citep{Fogel1988, He2019, Li2022} -->.<br>As a problem that has been studied for more than 40 years, bearing-only target motion estimation was originally studied to estimate the motion of ships on the ocean surface<!--  \citep{hoelzer1978modified} -->, and regained increasing research attention in recent years in vision-based target estimation tasks<!--  \citep{Ponda2009, Anjaly2018, He2019} -->.</p><p>&emsp;&emsp;Bearing-only target motion estimation requires an <em>observability condition</em>: The observer must have higher-order motion than the target and, more importantly, the higher-order motion must contain components that are orthogonal to the target’s bearing vector<!--  \citep{Fogel1988} -->.<br>Motivated by this observability condition, enormous works have studied how an observer should move to enhance the observability<!--  \citep{Hammel1989, Sabet2016, Anjaly2018, He2019} -->.<br>For instance, in our recent work<!--  \citep{Li2022} -->, we proposed a helical guidance law so that a MAV moves along a helical curve to optimize the observability in the 3D space.</p><p>&emsp;&emsp;A <em>limitation</em> of the observability condition of the classic bearing-only approach is that the observer must move in the lateral directions that are orthogonal to the bearing vector of the target.<br>Such additional lateral motion is usually unfavorable because it may conflict with the desired motion of the observer in many tasks.<br>For example, in an aerial target pursuit task, the pursuer is desired to approach the target as fast as possible and then keep stationary relative to the target. Then, the additional lateral motion would conflict with the desired motion.<br>It is, therefore, important to study other ways that can enhance the observability while avoiding unfavorable lateral motion.</p><p>&emsp;&emsp;The second type of useful information of a bounding box is its <em>size</em> (either width or height).<br>The size of a bounding box is jointly determined by several factors such as the target’s distance, the target’s physical size, and the orientation of the camera.<br>The target’s physical size is usually unknown in many tasks, especially in those antagonistic ones such as aerial pursuit of misused MAVs.<br>As a result, the size of the bounding box cannot directly infer the target’s distance.<br>Nevertheless, it carries valuable information for localizing the target.</p><p>&emsp;&emsp;Surprisingly, the size information of the bounding box has not been well explored so far.<br>The work that is closely relevant to ours is the state-of-the-art one in<!--  \citep{Griffin2021} -->, where the size of a bounding box is used to localize unknown target objects.<br>Although the approach in<!--  \citep{Griffin2021} --> is inspiring, it relies on two assumptions: The target objects are stationary and the camera can only translate without rotating.<br>It is still an open problem how to estimate a target’s motion when the two assumptions are not valid.<br>Moreover, the theoretical role of the size of a bounding box in target motion estimation has not been fully understood so far.<br>Although the work in<!--  \citep{Vrba2020} --> also utilizes the size of the bounding box to estimate the target’s position, it is assumed that the target’s physical size is known in advance.</p><p>&emsp;&emsp;Estimating the motion of moving objects is also a fundamental problem in dynamic SLAM.<br>For example, the works in<!--  \citep{Yang2019,Qiu2019} --> firstly estimate the camera’s pose and secondly estimate the target object’s pose subject to a scale factor, and finally estimate the scale factor from multi-view measurements.<br>To estimate the target object’s pose subject to a scale factor,<!--  \citep{Yang2019} --> and<!--  \citep{Qiu2019} --> rely on detecting, respectively, a 3D bounding box and sufficient feature points inside the 2D bounding box.<br>Different from<!--  \citep{Yang2019,Qiu2019} -->, our proposed approach merely utilizes a 2D image bounding box without further extracting feature points or a 3D bounding box inside the 2D bounding box.<br>As a result, one benefit is that this approach is more computationally efficient.<br>Moreover, this approach can handle the challenging small-target case where the target object is far and hence its image is small.<br>In this case, it would be unreliable to extract sufficient stable features or conduct 3D detection.</p><p>&emsp;&emsp;The aforementioned approaches in<!--  \citep{Griffin2021,Yang2019,Qiu2019} --> are all based on multiple views.<br>It is also possible to estimate the target’s depth from a single view/image<!--  \citep{Tekin2018, Vrba2020} -->.<br>The single-view approach however requires prior information of the objects.<br>Moreover, it would be unable to successfully localize target objects with different sizes but similar appearances.<br>In this paper, we focus on the multi-view case.</p><p>&emsp;&emsp;In this paper, we propose a novel <em>bearing-angle</em> target motion estimation approach that models a bounding box as bearing-angle measurements.<br>This approach can enhance the observability by fully exploiting the information in a bounding box rather than relying on the additional lateral motion of the observer.<br>The benefit of the proposed bearing-angle approach comes with no additional cost since the bounding box is a standard output of object detection algorithms.<br>The approach simply exploits the angle information that has not been fully exploited in the past.<br>No additional sensing devices or special detection algorithms are required.</p><p>&emsp;&emsp;The technical novelties of this approach are threefold.</p><ol><li><p>The proposed approach does not directly use the size of a bounding box because the size is variant to the orientation of the camera.<br>That is, even if the target’s relative position is unchanged, the size of the bounding box still varies when the camera rotates.<br>Motivated by this problem, we convert the size of the bounding box to an angle subtended by the target (see <a href="#fig_architecture_outdoor">Fig.1</a>).<br>The merit of using the angle measurement is that it is <em>invariant</em> to the camera’s orientation change (see <a href="#fig_cam_rotate">Fig.2</a>) and hence can greatly facilitate the estimator design.<br>In this way, the assumption in<!--  \citep{Griffin2021} --> that the camera can only translate but not rotate can be avoided.</p></li><li><p>Although the bearing-angle approach incorporates an additional angle measurement, it is nontrivial to see how to properly use this measurement because the angle does not directly infer the target’s distance given that the target’s size is unknown.<br>We notice that the angle is a joint nonlinear function of the target’s physical size and relative distance.<br>Hence, the state vector, which only consists of the target’s position and velocity in the conventional bearing-only approach, is augmented by the unknown target’s physical size.<br>Since the bearing and angle measurements are all nonlinear functions of the target’s state, we establish a pseudo-linear Kalman filter to properly utilize the measurements to enhance estimation stability.<br>Both simulation and real-world experiments verify the effectiveness of the proposed estimator.</p></li><li><p>Although an additional angle measurement is used, an additional unknown, the target’s physical size, is also introduced into the estimator.<br>It is, therefore, nontrivial to see how the additional angle measurement can help improve the observability.<br>Motivated by this problem, we prove the necessary and sufficient observability condition for bearing-angle target motion estimation.<br>In particular, we show that the target’s motion can be recovered if and only if the observer has a higher-order motion than the target.<br>Different from the bearing-only case, the higher-order motion is <em>not</em> required to be in the lateral directions that are orthogonal to the bearing vector.<br>This is an important enhancement of the observability. As we show in various experiments, the bearing-angle approach can successfully recover the target’s motion in many scenarios where the bearing-only approach fails.</p></li></ol><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Algorithms-for-bearing-only-target-motion-estimation"><a href="#Algorithms-for-bearing-only-target-motion-estimation" class="headerlink" title="Algorithms for bearing-only target motion estimation"></a>Algorithms for bearing-only target motion estimation</h3><p>&emsp;&emsp;Bearing-only target motion analysis aims to estimate the target’s motion states, such as position and velocity, using bearing measurement only.<br>It was originally motivated by ship localization and tracking in the ocean<!--  \citep{hoelzer1978modified} -->. With the rapid development of small-scale mobile robots equipped with cameras, the bearing-only approach regained increasing attention in recent years<!--  \citep{Ponda2009, Anjaly2018, He2019} -->.</p><p>&emsp;&emsp;Kalman filter-based estimators are widely used in the bearing-only target motion.<br>One challenge of applying the Kalman filter to the bearing-only estimation is the nonlinearity of the bearing measurement.<br>The conventional extended Kalman filter (EKF) exhibits divergence problems when applied to bearing-only target motion estimation<!--  \citep{Aidala1979, Lin2002} -->.<br>Several methods have been proposed to solve this problem.<br>They can be divided into two types.<br>The first type is the modified polar EKF, which was first proposed in<!--  \citep{hoelzer1978modified} -->.<br>In this approach, three observable quantities are separated from the unobservable ones to prevent divergence.<br>The work in<!--  \citep{Stallard1991} --> extends this approach to the case of spherical coordinates to track targets in 3D space.<br>The second type is the pseudo-linear Kalman filter, which is first proposed in<!--  \citep{Lingren1978} --> to solve the instability problem by transforming the nonlinear measurement equation into a pseudo-linear one.<br>However, this transformation makes the noise become non-Gaussian and highly correlated to the measurement matrix and then causes estimation bias.<br>Nevertheless, the work in<!--  \citep{Aidala1982} --> theoretically proves that the velocity estimation has no bias, and the position estimation bias can be removed by the observer’s maneuvers.</p><p>&emsp;&emsp;Recently, other estimation algorithms based on advanced but more complex filters have been proposed.<br>The work in<!--  \citep{Farina1999} --> uses the maximum likelihood (MLE) algorithm to estimate the target’s motion using bearing-only measurements.<br>The comparison with the Cramer-Rao lower bound indicates that the MLE-based estimator is effective against measurement errors.<br>The work in<!--  \citep{Dogancay2005} --> proposes a constrained total least-squares algorithm, which can improve the estimation accuracy when the error of bearing measurement is large.<br>Three different algorithms are used and compared in<!--  \citep{Lin2002} -->.<br>The results show that the EKF, the pseudo-linear filter, and the particle filter have similar performances in most situations, while the EKF loses track when the initial estimate error is large.</p><p>&emsp;&emsp;Another type of approach, called bearing-only trajectory triangulation<!--  \citep{Avidan2000} -->, estimates the target’s position from the perspective of trajectory fitting.<br>It reconstructs the trajectory by intersecting parametric trajectory to a series of sight rays obtained from bearing measurement.<br>Once the trajectory is successfully fitted, the target’s position at each time instant can be estimated by the intersection of the bearing and the trajectory.<br>The trajectory fitting relies on the assumption of the trajectory’s shape.<br>However, in many applications, the target’s trajectory is complex and unknown in advance.<br>Many consecutive studies aim to relax this assumption in various ways based on hypersurfaces<!--  \citep{Kaminski2004} -->, parametric temporal polynomials<!--  \citep{Yu2009} -->, or compact basis vectors<!--  \citep{Park2015} -->.</p><h3 id="Observability-analysis-of-bearing-only-target-motion-estimation"><a href="#Observability-analysis-of-bearing-only-target-motion-estimation" class="headerlink" title="Observability analysis of bearing-only target motion estimation"></a>Observability analysis of bearing-only target motion estimation</h3><p>&emsp;&emsp;Observability is a fundamental problem in bearing-only target motion estimation.<br>Early works mainly focus on whether the system is observable or not.<br>For example, the work in<!--  \citep{Lingren1978} --> uses the rank of observation matrix to determine the observability.<br>The work in<!--  \citep{Fogel1988} --> extends the observability criterion in<!--  \citep{Nardone1981} --> to the Nth-order target dynamics and inspires us for the observability analysis in Section<!--  \ref{sec_observability_criteria} --> <a href="#sec_observability_criteria">Observability Analysis by Solving Linear Equations</a>.<br>All these conditions indicate that the observer must have extra high-order motion in the lateral direction.<br>The observability condition can be significantly relaxed in our approach.</p><p>&emsp;&emsp;Unlike the works on determining whether the system is observable or not, some studies focus on quantifying the observability degree.<br>The work in<!--  \citep{Hammel1989} --> first introduces the Fisher information matrix (FIM) into the observability analysis.<br>The works in<!--  \citep{Sabet2016} --> and<!--  \citep{Anjaly2018} --> use FIM-based objective functions to maximize observability.<br>We also use the FIM in our former work<!--  \citep{Li2022} --> to optimize the 3D helical guidance law for better observability.<br>Another method called the geometric method uses the geometric relationship between the target and the observer in two consecutive time instants to derive the measure of observability<!--  \citep{He2019, Woffinden2009} -->, and the results are consistent with those derived using FIM.<br>Compared to the bearing-only approach, the observability degree of our bearing-angle method is sufficient to estimate the target’s motion in many common scenarios such as tracking and guidance (see experiment results in Figs.~\ref{fig_matlab_3} and~\ref{fig_outdoor_1}).</p><h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><p><a id= "fig_cam_rotate"></a><br><img src="https://picture.adunas.top/Article/arXiv-2401.17117v1/fig_cam_rotate.png" alt="Fig.2 The size of the bounding box varies when the camera rotates. By contrast, the angle subtended by the target object is invariant to the camera&#39;s orientation change."></p><p>&emsp;&emsp;Consider a target object moving in the 3D space. Its position and velocity at time $t_k$ are denoted as $p_T(t_k) \in \mathbb{R}^3$ and $v_T(t_k) \in \mathbb{R}^3$, respectively.<br>Suppose there is an observer carrying a monocular camera to observe the target.<br>The position of the observer is denoted as $p_o(t_k) \in \mathbb{R}^3$.<br>Here, we assume that the observer/camera’s pose including its position and orientation can be obtained in other ways.<br>For example, it can be measured directly by RTK GPS<!--  \citep{Li2022} --> or estimated by visual inertial odometry<!--  \citep{Qiu2019} -->.<br>In the rest of the paper, the dependence of a variable on $t_k$ is dropped when the context is clear.</p><p>&emsp;&emsp;If the target object can be detected by a vision algorithm, we can obtain a bounding box surrounding the target object in the image.<br>Two types of information carried by the bounding box can be used to estimate the motion of the target.</p><p>&emsp;&emsp;First, the center point of the bounding box can be used to calculate the <em>bearing</em> vector of the target.<br>In particular, denote $g \in \mathbb{R}^3$ as the unit bearing vector pointing from $p<em>o $ to $p_T $.<br>Suppose $P</em>\text{cam}\in \mathbb{R}^{3\times3}$ is the intrinsic parameter matrix of the camera<!--  \citep[Section~\ref{section_bearing-angle-target-motion-estimator}]{Ma2012} -->, and $R_\text{c}^\text{w} \in \mathbb{R}^{3\times 3}$ is the rotation from the camera frame to the world frame.<br>Then, the bearing vector $g$ can be calculated as</p><script type="math/tex; mode=display">\begin{align*}g =\dfrac{R_\text{c}^\text{w}P_\text{cam}^{-1}q_{\rm pix}}{\|R_\text{c}^\text{w}P_\text{cam}^{-1}q_{\rm pix}\|},\end{align*}%\label{eq_bearing_information}</script><p>where $q<em>{\rm pix} =[x</em>{\rm pix} , y<em>{\rm pix} , 1]^\mathrm{T} \in \mathbb{R}^3$.<br>Here, $(x</em>{\rm pix} ,y_{\rm pix})$ is the pixel coordinate of the center point of the bounding box.</p><p>&emsp;&emsp;Second, the size of the bounding box can be used to calculate the <em>angle</em> subtended by the target in the camera’s field of view.<br>The reason that we convert the bounding box’s size to the angle is that the angle is invariant to the camera’s orientation change (see <a href="#fig_cam_rotate">Fig.2</a>).<br>In particular, let $s<em>{\rm pix} $ denote the size of the bounding box.<br>It can be either the width or the height.<br>Let $\theta \in (0,\pi/2)$ be the angle.<br>According to the pin-hole camera model<!--  \citep[Section~\ref{section_bearing-angle-target-motion-estimator}]{Ma2012} --> and the law of cosine (see <a href="#fig_cam_rotate">Fig.2</a>), the angle can be calculated as<br>\begin{align*}<br>\theta = \arccos\left(\dfrac{l</em>\mathrm{left}^2 + l<em>\mathrm{right}^2 - s</em>\mathrm{pix}^2}{2l<em>\mathrm{left}l</em>\mathrm{right}}\right),<br>\end{align*}%\label{eq<em>angle_information}<br>where $l</em>\mathrm{left}=\sqrt{(f/\alpha)^2+(\delta x-s<em>\mathrm{pix}/2)^2+\delta y^2}\in\mathbb{R}$ and $l</em>\mathrm{right}=\sqrt{(f/\alpha)^2+(\delta x+s<em>\mathrm{pix}/2)^2+\delta y^2}\in\mathbb{R}$ are the distances in pixel from the camera center to the middle points of the left and right sides of the bounding box, respectively (Fig.~\ref{fig_architecture_outdoor}).<br>Moreover, $f$ and $\alpha$ denote the camera’s focal length and single pixel size, respectively.<br>$i</em>{\text{width}}$ and $i<em>{\text{height}}$ represent the width and the height of the whole image in pixels, respectively. $\delta x=|x</em>\text{pix}-i<em>\text{width}/2|\in\mathbb{R}$ and $\delta y = |y</em>\text{pix}-i_\text{height}/2|\in\mathbb{R}$ are the distances between the center of the bounding box and the center of the image.</p><p>\begin{figure<em>}[!t]<br>    \centering<br>    \includegraphics[width=1\linewidth]{fig_architecture_algorithm}<br>    \caption{The architecture of the proposed approach. All the simulation and real-world experiments in this paper follow this architecture.}<br>    \label{fig_architecture_algorithm}<br>\end{figure</em>}</p><p>&emsp;&emsp;Our goal is to estimate the target’s position and velocity, $p_T$ and $v_T$, based on the noisy measurements of the bearing vector $g$ and the angle $\theta$ together with the observer’s own position $p_o$.<br>To achieve this goal, we propose a new bearing-angle target motion estimator (Fig.~\ref{fig_architecture_algorithm}).<br>The estimator is introduced in detail in Section~\ref{section_bearing-angle-target-motion-estimator}.<br>The observability of this estimator is analyzed based on Kalman’s observability criterion in Section~\ref{sec_analysis_of_observability_matrix}.<br>We further prove a necessary and sufficient observability condition of the observer in Section~\ref{sec_observability_criteria}.<br>Numerical simulation results are given in Section~\ref{sec_matlab_simulation}.<br>More realistic AirSim simulation results are given in Section~\ref{sec_airsim_simulation}.<br>Finally, real-world experiments are given in Section~\ref{sec_real_world_experimental_validation}.</p><p>\section{Bearing-Angle Target Motion Estimator}<br>\label{section_bearing-angle-target-motion-estimator}</p><p>This section designs a bearing-angle target motion estimator based on the framework of pseudo-linear Kalman filtering. The key here is to establish appropriate measurement and state transition equations.</p><h3 id="States-transition-equation"><a href="#States-transition-equation" class="headerlink" title="States transition equation"></a>States transition equation</h3><p>\label{sec_states_transition_equation}<br>The state vector of the target is designed as<br>\begin{align<em>}<br>x=<br>\left[<br>  \begin{array}{c}<br>    p_T \<br>    v_T \<br>    \ell \<br>  \end{array}<br>\right]\in \mathbb{R}^7,<br>\end{align</em>}%\label{eq_states_target}<br>where $p_T $ and $v_T $ are target’s global position and velocity, respectively.<br>Here, $\ell&gt;0$ is a scalar that represents the physical size of the target object in the dimension that is orthogonal to the bearing vector (see <a href="#fig_cam_rotate">Fig.2</a>). In this paper, $\ell$ is assumed to be constant or varying slowly, which means that the physical size of the target object should be approximately invariant from different viewing angles. Here, $\ell$ corresponds to $\theta$, which further corresponds to either the width or height of the bounding box. Whether $\ell$ should correspond to the width or height depends on in which dimension the physical size of the target object is invariant when viewed from different angles.<br>More explanation is given in Section~\ref{sec_dynamical_model_of_size}.</p><p>&emsp;&emsp;Different from the bearing-only case where the state merely consists of the position and velocity, the state here is augmented by the target’s physical size. This is due to the fact that the angle measurement is a function of the target’s physical size, which should be estimated as well. One may wonder whether the state vector can also incorporate the target’s acceleration. To estimate high-order motion (e.g., acceleration) of the target, the observer must have higher-order motion (e.g., nonzero jerk) according to the observability condition presented in Section~\ref{sec_observability_criteria}. Otherwise, the estimation would diverge. Therefore, it is preferred to exclude the acceleration and merely estimate the position and velocity.</p><p>&emsp;&emsp;If no information of the target’s motion is available, it is common to model the target’s motion as a discrete-time noise-driven double integrator:</p><script type="math/tex; mode=display">\begin{align}    x(t_{k+1})=Fx(t_k) +q(t_k) ,\end{align}$$\label{eq_state_transition}where</script><p>\begin{align}<br>F=<br>\begin{bmatrix}<br>I<em>{3\times3} &amp; \delta tI</em>{3\times3} &amp; 0<em>{3\times1}  \<br>0</em>{3\times3} &amp; I<em>{3\times3}  &amp; 0</em>{3\times1}   \<br>0<em>{1\times 3} &amp; 0</em>{1\times 3} &amp; 1<br>\end{bmatrix}\in\mathbb{R}^{7\times 7},<br>\end{align}</p><script type="math/tex; mode=display">\label{eq_matrix_A}with $\delta t$ as the sampling time, and $I$ and $0$ as the identity and zero matrices, respectively.Here, $q \in\mathbb{R}^7$ is a zero-mean process noise satisfying $q \sim \mathcal{N}(0,{\Sigma}_q)$, where the covariance matrix is</script><p>\begin{align}<br>{\Sigma}<em>q=\text{diag}(0, 0, 0, \sigma_v^2, \sigma_v^2, \sigma_v^2, \sigma</em>\ell^2)\in\mathbb{R}^{7\times7}.<br>\end{align}%\label{eq_covariance_q}</p><script type="math/tex; mode=display">Here, $\sigma_v\in\mathbb{R}$ and $\sigma_\ell\in\mathbb{R}$ are the standard deviations of the target's velocity and size, respectively.When the target's shape is irregular, $\ell$ may vary when viewed from different angles.By letting $\sigma_\ell\ne0$, we can handle the case where $\ell$ varies slowly.The dynamic modeling of $\ell$ is discussed in the following subsection.\subsection{Dynamic modeling of target's physical size}\label{sec_dynamical_model_of_size}&emsp;&emsp;Since the target's physical size $\ell$ is a state variable to be estimated, it is important to discuss its dynamic model. In fact, the dynamic model of $\ell$ in \eqref{eq_state_transition} assumes that $\ell$ varies slowly. We next justify this modeling and provide more discussion.First of all, $\ell$ corresponds to the physical size of the target object in the dimension that is orthogonal to the bearing vector. Its dynamics can be categorized into three cases.*1) $\ell$ is invariant.*In theory, when $\ell$ is invariant, a change of $\theta$ implies a change of $r$.As a result, the measurement of $\theta$ can help improve the system's observability, as proven in Section~\ref{sec_observability_criteria}.An ideal case where $\ell$ is invariant is that the target object is a sphere or cylinder so that $\ell$ corresponds to its diameter<!--  \citep{Vrba2020} -->.In practice, the target object does not have to be the ideal case. For example, consider an autonomous driving scenario where a focal vehicle uses a camera to localize its surrounding vehicles in the 2D plane.Although the physical size of a surrounding vehicle changes greatly when viewed from behind or side, the height of the vehicle is *invariant* from different side-view angles.In this case, $\ell$ corresponds to the height of the vehicle, and we need to use the height of the image bounding box to calculate $\theta$.*2) $\ell$ varies slowly.*If there does not exist any dimension in which the physical size of the target remains invariant, $\ell$ may vary slowly when the target is viewed from different angles. For example, in the tasks of aerial target pursuit, if the target is a quadcopter or hexacopter, then $\ell$ is approximately equal to the wheelbase but may vary slightly when viewed from different angles since the MAV is not a perfect cylinder.In this case, $\ell$ corresponds to the wheelbase of the MAV, and we need to use the width of the image bounding box to calculate $\theta$.If $\ell$ varies slowly, it can still be treated as invariant within short time intervals.As long as the observability condition (Section~\ref{sec_observability_criteria}) is satisfied, the motion of the target as well as $\ell$ can be successfully estimated.This fact is supported by the experimental results in Section~\ref{sec_sim_res_circular_scenario}.It is however worth nothing that the performance of the proposed bearing-angle approach would degenerate to the conventional bearing-only one because the additional information brought by $\theta$ is used to estimate the time-varying $\ell$ rather than helping improve the system's observability.*3) $\ell$ varies rapidly.*If $\ell$ varies rapidly due to certain reasons, it would be difficult to distinguish whether the change of $\theta$ is caused by the change of $\ell$ or the change of $r$.For example, when a MAV is used to track a ground vehicle, $\ell$ in any dimension may vary rapidly when the relative motion between the MAV and the ground vehicle is highly dynamic.In such scenarios, the additional information brought by $\theta$ is no longer sufficient to estimate the rapidly varying $\ell$ in this case. Additional visual information such as a 3D bounding box that indicates the target's 3D attitude is required. This is an important topic for future research but out of the scope of the present paper.\subsection{Nonlinear measurement equations}The bearing vector $g$ and the subtended angle $\theta $ are both nonlinear functions of the target's position. In particular,</script><p>\begin{align}<br>    g &amp;=\dfrac{p_T -p_o }{r },<br>    \theta &amp;=2\arctan\left(\dfrac{\ell}{2r }\right)\approx \dfrac{\ell}{r },<br>\end{align}</p><script type="math/tex; mode=display">\label{eq_information}    \label{eq_bearing_measure} \\\label{eq_theta_measure}where$$r =\|\my{p}_T -\my{p}_o \|</script><p>is the distance between the target and the observer.<br>It is notable that there is an approximation in \eqref{eq_theta_measure}. This approximation is accurate.<br>Specifically, when $r&gt;3\ell$, which is common in practice, it can be verified that the approximation error is less than $0.08\%$. The approximation error further decreases as $r$ increases.</p><p>In practice, measurements always contain noises.<br>First, denote $\hat{\my{g}} \in\mathbb{R}^3$ as the noise-corrupted bearing measurement. Then, we have<br>\begin{align}<br>\label{eq<em>noised_g_mear}<br>\hat{\my{g}}  = \my{R}\left(\my{\eta} , \epsilon \right) \my{g} ,<br>\end{align}<br>where $\my{R}\left(\my{\eta} , \epsilon \right) \in \mathbb{R}^{3\times 3}$ is a rotation matrix that perturbs $\my{g}$.<br>Here, $\my{\eta} \in\mathbb{R}^3$ is a unit vector representing a random rotation axis, and $\epsilon \in \mathbb{R}$ is a random rotation angle.<br>This rotation matrix would rotate the vector $\my{g} $ by an angle $\epsilon $ around the axis $\my{\eta} $.<br>The productive noise in \eqref{eq_noised_g_mear} can be transformed into an additive one:<br>\begin{align}\label{eq_noised_g_mear_add}<br>    \hat{\my{g}}  = \my{g}  + \my{\mu} ,<br>\end{align}<br>where $\my{\mu} =(\my{R}\left(\my{\eta} , \epsilon \right) - \my{I}</em>{3\times3})\my{g} \in\mathbb{R}^3$ is the measurement noise of the bearing vector.<br>The covariance of $\mu$ is derived in our previous work<!--  \citep{Li2022} -->. Since the covariance is complex and involves unknown true values, we can approximately treat it as a Gaussian noise: $\mu\sim\mathcal{N}(0, \sigma<em>\mu^2 I</em>{3\times 3})$<!--  \citep{Li2022} -->.</p><p>Substituting \eqref{eq_bearing_measure} into \eqref{eq_noised_g_mear_add} gives the <em>nonlinear bearing measurement equation:</em><br>\begin{align}\label{eq_bearing_measure_noise}<br>    \hat{\my{g}} &amp;=\dfrac{\my{p}_T -\my{p}_o }{r } + \my{\mu} .<br>\end{align}</p><p>Second, denote $\hat{\theta} \in\mathbb{R}$ as the noise-corrupted measurement of the subtended angle. Then, we have<br>\begin{align}\label{eq_noise_theta}<br>    \hat{\theta} =\theta  + w ,<br>\end{align}<br>where $w \sim \mathcal{N}(0, \sigma^2_w)$ is the measurement noise.<br>Substituting \eqref{eq_theta_measure} into \eqref{eq_noise_theta} yields the <em>nonlinear angle measurement equation:</em><br>\begin{align}\label{eq_theta_measure_noise}<br>    \hat{\theta} &amp;=\dfrac{\ell}{r } + w.<br>\end{align}</p><p>\subsection{Pseudo-linear measurement equations}</p><p>The measurement equations \eqref{eq_bearing_measure_noise} and \eqref{eq_theta_measure_noise} are nonlinear in the target’s state. In the following, we convert the two equations to be pseudo-linear and then apply pseudo-linear Kalman filtering to achieve better estimation stability<!--  \citep{Lin2002} -->.</p><p>First, to convert the 3D bearing measurement to pseudo-linear, we introduce a useful orthogonal projection matrix:<br>\begin{align<em>}<br>    \my{P}<em>{\hat{\my{g}} }\doteq\my{I}</em>{3\times 3}-\hat{\my{g}} \hat{\my{g}}^\mathrm{T}  \in \mathbb{R}^{3\times 3}.<br>\end{align</em>}%\label{eq<em>projMatrix}<br>This matrix plays an important role in the analysis of bearing-related estimation and control problems<!--  \citep{Zhao2019} -->. It has an important property: $$\my{P}</em>{\hat{\my{g}} }\hat{\my{g}} =\my{0}<em>{3\times 1}.$$<br>As a result, multiplying $r\my{P}</em>{\hat{\my{g}} }$ on both side of \eqref{eq<em>bearing_measure_noise} yields<br>\begin{align*}<br>\my{0}</em>{3\times 1}=\my{P}<em>{\hat{\my{g}} }(\my{p}_T -\my{p}_o) + r\my{P}</em>{\hat{\my{g}} }\my{\mu}<br>\end{align<em>}<br>and consequently<br>\begin{align</em>}<br>\my{P}<em>{\hat{\my{g}} }\my{p}_o =\my{P}</em>{\hat{\my{g}} }\my{p}<em>T  + r\my{P}</em>{\hat{\my{g}} }\my{\mu}.<br>\end{align<em>}%\label{eq_g_pseudo_linear_measurement}<br>Rewriting this equation in terms of the target’s state variables yields the </em>pseudo-linear bearing measurement equation:*<br>\begin{align}\label{eq<em>pseudo_linear_measurement_g_equation}<br>\my{P}</em>{\hat{\my{g}} }\my{p}<em>o =<br>\begin{bmatrix}<br>\my{P}</em>{\hat{\my{g}} } &amp;<br>\my{0}<em>{3\times4}<br>\end{bmatrix}<br>\left[<br>  \begin{array}{c}<br>    p_T \<br>    v_T \<br>    \ell \<br>  \end{array}<br>\right]  +  r\my{P}</em>{\hat{\my{g}} }\my{\mu} .<br>\end{align}<br>Here, $\my{P}_{\hat{\my{g}} }\my{p}_o $ on the left-hand side is the new measurement, which is pseudo-linear in the target’s state variables.<br>The reason that it is called “pseudo” is because the measurements also appear on the right-hand side of the equation, especially in the measurement matrix.</p><p>Second, we convert the nonlinear angle measurement in \eqref{eq<em>theta_measure_noise} to be pseudo-linear.<br>To that end, multiplying $r \my{\hat{g}} $ on both side of \eqref{eq_theta_measure_noise} yields<br>\begin{align}\label{eq_theta_pseudo_tem}<br>\hat{\theta} r\hat{\my{g}}  = \ell\hat{\my{g}} +wr\hat{\my{g}} .<br>\end{align}<br>It follows from \eqref{eq_bearing_measure_noise} that $r\my{\hat{g}}=\my{p}_T -\my{p}_o+r\mu$, substituting which into the left-hand side of \eqref{eq_theta_pseudo_tem} gives<br>\begin{align<em>}<br>\hat{\theta} (\my{p}_T -\my{p}_o+r\mu)  = \ell\hat{\my{g}} +wr\hat{\my{g}}.<br>\end{align</em>}<br>Reorganizing the above equation gives<br>\begin{align<em>}<br>\hat{\theta} \my{p}_o  = &amp;\hat{\theta} \my{p}_T  - \ell\hat{\my{g}} + r(\hat{\theta}  \my{\mu}  - w \hat{\my{g}}).<br>\end{align</em>}<br>Rewriting this equation in terms of the target’s state variables yields the <em>pseudo-linear angle measurement equation:</em><br>\begin{align}\label{eq_pseudo_linear_measurement_theta_equation}<br>\begin{aligned}<br>\hat{\theta} \my{p}_o  =&amp;<br>\begin{bmatrix}<br>\hat{\theta} \my{I}</em>{3\times 3} &amp; \my{0}_{3\times 3}  &amp; -\hat{\my{g}}<br>\end{bmatrix}<br>\left[<br>  \begin{array}{c}<br>    p_T \<br>    v_T \<br>    \ell \<br>  \end{array}<br>\right]</p><ul><li>r(\hat{\theta}  \my{\mu}  - w \hat{\my{g}} ),<br>\end{aligned}<br>\end{align}<br>where $\hat{\theta} \my{p}_o $ is the new measurement that is pseudo-linear in the target’s state variables.</li></ul><p>\subsection{Bearing-angle estimation algorithm}</p><p>Combining  \eqref{eq<em>pseudo_linear_measurement_g_equation} and \eqref{eq_pseudo_linear_measurement_theta_equation} gives the compact form of the measurement equation:<br>\begin{align}\label{eq_pseudo_linear_measurement_equations}<br>\my{z} = \my{H} \my{x}  + \my{\nu} ,<br>\end{align}<br>where<br>\begin{subequations}<br>\begin{align}<br>\my{z} &amp;=<br>    \begin{bmatrix}<br>    \my{P}</em>{\hat{g}} \my{p}<em>o   \<br>    \hat{\theta} \my{p}_o<br>    \end{bmatrix}\in\mathbb{R}^6, \<br>\my{H}&amp; =<br>    \begin{bmatrix}<br>    \my{P}</em>{\hat{g}}  &amp; \my{0}<em>{3\times 3} &amp; \my{0}</em>{3\times 1} \<br>    \hat{\theta} \my{I}<em>{3\times 3} &amp; \my{0}</em>{3\times 3}  &amp; -\hat{\my{g}}<br>    \end{bmatrix}\in\mathbb{R}^{6\times7},<br>    \label{eq<em>matrix_H}    \<br>\my{\nu}  &amp;=<br>    \begin{bmatrix}<br>    r \my{P}</em>{\hat{g}} \my{\mu}  \<br>    r (\hat{\theta}  \my{\mu}  - w \hat{\my{g}} )<br>    \end{bmatrix}<br>    \in\mathbb{R}^6.<br>    \label{eq<em>final_measurement_noise}<br>\end{align}<br>\end{subequations}<br>Here, $\nu$ can be rewritten as a matrix form<br>\begin{align<em>}<br>    \nu=E<br>    \begin{bmatrix}<br>        \mu \ w<br>    \end{bmatrix},<br>\end{align</em>}<br>where<br>\begin{align}\label{eq_E_mat}<br>    E=r<br>    \begin{bmatrix}<br>        P</em>{\hat{g}} &amp; 0<em>{3\times 1}\<br>        \hat{\theta}I</em>{3\times 3} &amp; -\hat{g}<br>    \end{bmatrix}\in\mathbb{R}^{6\times 4}.<br>\end{align}<br>As a result, $\nu$ can be approximately treated as a linear transformation of Gaussian noises.<br>Its covariance matrix can be calculated as<br>\begin{align<em>}%\label{eq<em>final_measurement_noise_covariance}<br>\my{\Sigma}</em>{\my{\nu}}  = E<br>\begin{bmatrix}<br>\sigma<em>\mu^2 I</em>{3\times 3} &amp; 0<em>{3\times1}\<br>0</em>{1\times 3} &amp; \sigma_w^2<br>\end{bmatrix}<br>E^\mathrm{T}\in\mathbb{R}^{6\times6}.<br>\end{align</em>}<br>Although the quantities in $E$ such as $\hat{g}$ and $\hat{\theta}$ contain measurement noises, it is a common practice to treat them as deterministic quantities. Otherwise, if, for example, $\hat{g}$ is split to $\hat{g}=g+\mu$ and we consider the noise separately, the expression of $\nu$ would be a complex function of the true values and the noises. Since the true values are unknown, the covariance cannot be calculated.<br>Moreover, $r $ in \eqref{eq_E_mat} is the true target range, which is unknown. We can use the estimated value $\hat{r} =|\hat{\my{p}}_T -\my{p}_o |$ to replace it in implementation. Here, $\hat{p}_T\in\mathbb{R}^3$ is the estimated value of the target’s position. This technique has been used in bearing-only target estimation \citep{He2018, Li2022}.</p><p>With the state transition equation \eqref{eq<em>state_transition} and the measurement equation \eqref{eq_pseudo_linear_measurement_equations}, the bearing-angle estimator can be realized by the Kalman filter.<br>For a quick reference, we list the steps below.<br>The prediction steps are<br>\begin{align*}<br>\hat{\my{x}}^{-}(t_k) &amp;= \my{F}\hat{\my{x}}(t</em>{k-1}), \<br>\my{P}^{-}(t<em>k) &amp;= \my{F}\my{P}(t</em>{k-1})\my{F}^\mathrm{T} + \my{\Sigma}<em>q,<br>\end{align<em>}<br>where $\hat{\my{x}}^{-}(t_k)\in\mathbb{R}^7$ and $\my{P}^{-}(t_k)\in\mathbb{R}^{7\times7}$ are the prior estimated state and covariance matrix, respectively.<br>The correction steps are<br>\begin{align</em>}<br>\my{K}(t_k) &amp;= \my{P}^{-}(t_k)\my{H}^\mathrm{T}(t_k)\left[\my{H}(t_k)\my{P}^{-}(t_k)\my{H}^\mathrm{T}(t_k)+\my{\Sigma}</em>\nu\right]^{\dagger}, \<br>\hat{\my{x}}(t<em>k) &amp;= \hat{\my{x}}^{-}(t_k) + \my{K}(t_k)\left[\my{z}(t_k)-\my{H}(t_k)\hat{\my{x}}^{-}(t_k)\right], \<br>\my{P}(t_k) &amp;=\left[\my{I}</em>{7\times 7} -\my{K}(t<em>k)\my{H}(t_k) \right]\my{P}^{-}(t_k),<br>\end{align*}<br>where $\my{K}(t_k)\in\mathbb{R}^{7\times6}$ is the Kalman gain matrix, $\hat{\my{x}}(t_k) $  and $\my{P}(t_k)$ are posterior estimated state and covariance matrix, and symbol $\dagger$ denotes the pseudoinverse.<br>The usage of pseudoinverse in the Kalman filter is a common practice to prevent the situation that $\my{H}(t_k)\my{P}^{-}(t_k)\my{H}^\mathrm{T}(t_k)+\my{\Sigma}</em>\nu$ is rank deficient \citep{YOSHIKAWA1972,Kulikov2018}.</p><p>\section{Observability Analysis by Kalman’s Criterion}\label{sec_analysis_of_observability_matrix}</p><p>Although an additional angle measurement is adopted in the bearing-angle estimator, it is nontrivial to see whether this additional measurement can improve the system’s observability because an additional unknown variable, the target’s physical size, is also required to estimate. It is therefore necessary to study the observability conditions under which the target’s motion can be successfully estimated.</p><p>In this and the next sections, we present two methods to analyze the observability conditions. The first method, as presented in this section, relies on Kalman’s observability criterion, which is to check the rank of the observability matrix of a linear system. The second method, as presented in the next section, relies on solving a set of linear equations.<br>Both methods have been adopted in the literature to analyze the observability of estimators \citep{Zhao2015, Fogel1988}.<br>For the bearing-angle estimator, the first method considers the specific dynamics of the filter but is not able to handle the case when the target’s motion has a higher order.<br>The second method can handle the high-order motion of the target but does not consider the dynamics of the filter. We will show that the conclusions given by the two methods are consistent.<br>In both of the methods, we consider the case where $\ell$ is invariant.</p><p>\subsection{The observability matrix}</p><p>Consider a time horizon of $k\geq 3$ consecutive steps.<br>The observability matrix of the system of \eqref{eq<em>matrix_H} and \eqref{eq_matrix_A} can be calculated as<br>\begin{align}\label{eq_Qo}<br>    \my{Q}=<br>    \begin{bmatrix}<br>    \my{H}(t_1) \<br>    \my{H}(t_2)\my{F} \<br>    \my{H}(t_3)\my{F}^2 \<br>    \cdots \<br>    \my{H}(t_k)\my{F}^{k-1} \<br>    \end{bmatrix}\in\mathbb{R}^{6k\times7}.<br>\end{align}<br>Substituting the expressions of $F$ and $H$ in \eqref{eq_matrix_A} and \eqref{eq_matrix_H} into \eqref{eq_Qo} yields<br>\begin{align*}<br>\my{Q}=<br>\left[<br>\begin{array}{ccc}<br>\my{P}_g(t_1) &amp; \my{0}</em>{3\times 3} &amp; \my{0}<em>{3\times 1} \<br>\theta(t_1)\my{I}</em>{3\times 3} &amp; \my{0}<em>{3\times 3}  &amp; -\my{g}(t_1) \<br>\hdashline<br>\my{P}_g(t_2) &amp; \delta t\my{P}</em>{g}(t<em>2) &amp; \my{0}</em>{3\times 1} \<br>\theta(t<em>2)\my{I}</em>{3\times 3} &amp; \delta t\theta(t<em>2)\my{I}</em>{3\times 3}  &amp; -\my{g}(t<em>2) \<br>\hdashline<br>\vdots &amp; \vdots &amp; \vdots \<br>\hdashline<br>\my{P}_g(t_k) &amp; (k-1)\delta t\my{P}</em>{g}(t<em>k) &amp; \my{0}</em>{3\times 1} \<br>\theta(t<em>k)\my{I}</em>{3\times 3} &amp; (k-1)\delta t\theta(t<em>k)\my{I}</em>{3\times 3}  &amp; -\my{g}(t<em>k)\<br>\end{array}<br>\right].<br>\end{align*}<br>Note that the noises in the bearing and angle measurements are neglected when we analyze the fundamental observability property.<br>After a series of elementary row transformations in $\my{Q}$, we can obtain<br>\begin{align}\label{eq_Qo_2}<br>\my{Q}<br>\rightarrow<br>\begin{bmatrix}<br>\my{I}</em>{3\times 3} &amp; \my{0}<em>{3\times 3} &amp; -\my{g}(t_1)/\theta(t_1) \<br>\my{0}</em>{3\times 3} &amp; \my{I}<em>{3\times 3} &amp; -\delta\my{v}(t_2)/\ell \<br>\vdots &amp; \vdots &amp; \vdots \<br>\my{0}</em>{3\times 3} &amp; \my{I}<em>{3\times 3} &amp; -\delta\my{v}(t_k)/\ell \<br>\hdashline<br>\my{0}</em>{3k\times 3} &amp; \my{0}<em>{3k\times 3} &amp; \my{0}</em>{3k\times 1}<br>\end{bmatrix},<br>\end{align}<br>where<br>\begin{align<em>}<br>\delta\my{v}(t_k) \doteq \my{v}_T(t_k) - \my{v}_o(t_k)<br>\end{align</em>}%\label{eq_delta_vel}<br>is the relative velocity.</p><p>In the following two subsections, we analyze the rank of the observability matrix in two scenarios where the observer moves with zero and nonzero acceleration, respectively. In the two scenarios, the target is always assumed to move with a constant velocity:<br>\begin{align<em>}<br>\my{v}_T(t_k) = \my{v}_T^\text{const}.<br>\end{align</em>}</p><p>\subsection{Case 1: the observer’s velocity is constant}<br>Denoted $\my{v}<em>o\in \mathbb{R}^3$ as the velocity of the observer.<br>Consider the case where the observer has a constant velocity $\my{v}_o^\text{case1}(t_i)=\my{v}_o^\text{const}$ for any $i\in{1,\dots,k}$.<br>Then, the relative velocity is also constant:<br>\begin{align}\label{eq_delta_vel_case1}<br>\delta \my{v}^\text{case1}(t_i) = \my{v}_T^\text{const} - \my{v}_o^\text{const} = \delta\my{v}^\text{const}.<br>\end{align}<br>Substituting \eqref{eq_delta_vel_case1} into \eqref{eq_Qo_2} and conducting elementary row transformation yields<br>\begin{align}\label{eq_Qo_3}<br>\my{Q}^\text{case1}<br>\rightarrow<br>\left[<br>\begin{array}{cc:c}<br>\my{I}</em>{3\times 3} &amp; \my{0}<em>{3\times 3} &amp; -\my{g}(t_1)/\theta(t_1) \<br>\my{0}</em>{3\times 3} &amp; \my{I}<em>{3\times 3} &amp; -\delta\my{v}^\text{const}/\ell \<br>\hdashline<br>\my{0}</em>{6(k-1)\times 3} &amp; \my{0}<em>{6(k-1)\times 3} &amp; \my{0}</em>{6(k-1)\times 1}<br>\end{array}\right].<br>\end{align}<br>Since the upper $6\times7$ block of \eqref{eq_Qo_3} has full row rank and the lower block is zero, the rank of $\my{Q}^\text{case1}$ is<br>\begin{align<em>}<br>\text{rank}\left(\my{Q}^\text{case1}\right) = 6.<br>\end{align</em>}<br>Since the number of states is seven and the rank is six, we know there is \emph{one unobservable mode}.<br>To identify this unobservable mode, we calculate the unobservable subspace, which is the null space of $\my{Q}$:<br>\begin{align}\label{eq_unobservable_subspace}<br>\text{Null}\left(\my{Q}^\text{case1}\right) = \text{span}\left{<br>\begin{bmatrix}<br>\my{g}(t_1)/\theta(t_1)  \<br>\delta\my{v}^\text{const}/\ell \<br>1<br>\end{bmatrix}\right}.<br>\end{align}<br>According to \eqref{eq_unobservable_subspace}, the unobservable mode is<br>\begin{align}<br>x^T<br>\left[<br>\begin{array}{c}<br>\my{g}(t_1)/\theta(t_1)  \<br>\delta\my{v}^\text{const}/\ell \<br>1<br>\end{array}<br>\right]<br>=<br>\my{p}_T^\mathrm{T}\dfrac{\my{g}(t_1)}{\theta(t_1)}+<br>\my{v}_T^\mathrm{T}\dfrac{\delta\my{v}^\text{const}}{\ell} + \ell.<br>\label{eq_unobservable_mode}<br>\end{align}%<br>Although there is only one unobservable mode, this mode given in \eqref{eq_unobservable_mode} involves all the states including the target’s position, velocity, and physical size. It suggests that the estimation of the three quantities is coupled. In conclusion, we know that, if the target moves with a constant velocity, its states are unobservable when the observer moves with a constant velocity.</p><p>\subsection{Case 2: the observer’s velocity is time-varying}</p><p>We now consider the case where the observer has nonzero acceleration so that its velocity is time-varying across the time horizon from $t_1$ to $t_k$.</p><p>Denote $\my{a}<em>o(t_i)\in\mathbb{R}$ as the observer’s acceleration, which can be approximated as<br>\begin{align}\label{eq_acc}<br>\my{a}_o(t_i) &amp;\approx<br>\dfrac{\my{v}_o(t_i) - \my{v}_o(t</em>{i-1})}{\delta t} \nonumber\<br>&amp;=-\dfrac{\left[\my{v}<em>T^\text{const} - \my{v}_o(t_i)\right] - \left[\my{v}_T^\text{const} - \my{v}_o(t</em>{i-1})\right]}{\delta t} \nonumber\<br>&amp;=-\dfrac{\delta \my{v}(t<em>i) - \delta \my{v}(t</em>{i-1})}{\delta t}.<br>\end{align}<br>Substituting \eqref{eq<em>acc} into \eqref{eq_Qo_2} and performing elementary row transformation yields<br>\begin{align}\label{eq_Q_case2_final}<br>\my{Q}^\text{case2}<br>\rightarrow<br>\left[\begin{array}{ccc}<br>\my{I}</em>{3\times 3} &amp; \my{0}<em>{3\times 3} &amp; -\my{g}(t_1)/\theta(t_1) \<br>\my{0}</em>{3\times 3} &amp; \my{I}<em>{3\times 3} &amp; -\delta\my{v}(t_2)/\ell \<br>\my{0}</em>{3\times 3} &amp; \my{0}<em>{3\times 3} &amp; \delta t \my{a}_o(t_3)/\ell \<br>\hdashline<br>\vdots &amp; \vdots &amp;\vdots \<br>\my{0}</em>{3\times 3} &amp; \my{0}<em>{3\times 3} &amp; \delta t \my{a}_o(t_k)/\ell \<br>\my{0}</em>{3k\times 3} &amp; \my{0}<em>{3k\times 3} &amp; \my{0}</em>{3k\times 1}<br>\end{array}\right].<br>\end{align}<br>The upper $6\times7$ block in \eqref{eq_Q_case2_final} has full column rank.<br>Therefore, if $a_o(t_i)\ne0$ for any $i\geq3$, then<br>\begin{align<em>}<br>\text{rank}\left(\my{Q}^\text{case2}\right) = 7,<br>\end{align</em>}<br>Which is the same as the number of estimated states.<br>Therefore, the target’s state is observable when the observer moves with nonzero acceleration.</p><p>\subsection{Summary of this section}</p><p>From the above analysis, we know that when the target has a constant velocity, its states including its position, velocity, and physical size are observable if and only if the observer has non-zero accelerations.</p><p>The critical difference of this condition from the bearing-only case is that the target’s states are still observable \emph{even if the observer moves along the bearing vector} towards or backward the target.<br>By contrast, for a bearing-only estimator, moving along the bearing vector is insufficient to recover the target’s motion. Therefore, the additional lateral motion of the observer required in the bearing-only case is \emph{not} required in the bearing-angle case anymore, which provides better flexibility for designing the observer’s motion.</p><p>\section{Observability Analysis by Solving Linear Equations}\label{sec_observability_criteria}</p><p>This section extends the observability condition obtained in the last section to more general cases where the target’s velocity does not have to be constant.</p><p>\subsection{Problem formulation}</p><p>The observability problem that we aim to solve is to determine whether $\my{p}_T(t)$ can be recovered from $\my{p}_o(t)$ and $g(t),\theta(t)$.</p><p>Suppose the target’s motion can be described by an $n$th-order polynomial during a time interval:<br>\begin{align}\label{eq<em>target_nth_Order}<br>    \my{p}_T(t)=\my{b}_0+\my{b}_1t+\cdots+\my{b}_nt^n,<br>\end{align}<br>where $\my{b}_0, \my{b}_1, \cdots, \my{b}_n\in\mathbb{R}^3$ are unknown constant vectors.<br>If we can determine the values of ${b_i}</em>{i=0}^n$, then we can determine the target’s motion and hence it is observable.<br>Although polynomials cannot represent all trajectories, they can effectively approximate a majority of them according to the method of Taylor expansion. This is especially true if we consider a short time horizon. This kind of technique has been adopted in the observability analysis of bearing-only target motion estimation tasks~\citep{Nardone1981, Lee2010}.</p><p>Suppose the observer’s motion is described by<br>\begin{align<em>}<br>    \my{p}_o(t)=\my{c}_0+\my{c}_1t+\cdots+\my{c}_nt^n+\my{h}(t),<br>\end{align</em>}%\label{eq<em>c_nth_Order}<br>where $\my{c}_0, \my{c}_1, \cdots, \my{c}_n\in\mathbb{R}^3$ are constant parameters, and<br>\begin{align}\label{eq_definition_h}<br>\my{h}(t) = \my{d}_1 t^{n+1}+\my{d}_2t^{n+2}+\cdots<br>\end{align}<br>represents \emph{higher-order} motion with $\my{d}_1, \my{d}_2, \cdots\in\mathbb{R}^3$.<br>It can be verified that the derivatives of $\my{h}(t)$ satisfy $\my{h}^{(i)}(0)=\my{0}</em>{3\times 1}$ for $i=0,1,\cdots, n$.<br>%<br>Let $\my{s}(t)\in\mathbb{R}^3$ be the relative motion between the target and the observer:<br>\begin{align}\label{eq_relative_motion}<br>    \my{s}(t)&amp;\doteq\my{p}_T(t)-\my{p}_o(t)  \nonumber\<br>    &amp;\doteq\my{s}_0+\my{s}_1t+\cdots+\my{s}_nt^n+\my{h}(t),<br>\end{align}<br>where $\my{s}_i = \my{d}_i - \my{c}_i\in\R^3$ for $i = 0,1,\cdots, n$.</p><p>If we can determine ${s<em>i}</em>{i=0}^n$, then $s(t)$ and hence $p<em>T(t)$ can be determined.<br>Therefore, we next study under what conditions ${s_i}</em>{i=0}^n$ can be uniquely determined.<br>Since $\my{p}_T(t)-\my{p}_o(t)=g(t)r(t)$ according to \eqref{eq_bearing_measure} and $r(t)=\ell/\theta(t)$ according to \eqref{eq_theta_measure}, we have</p><script type="math/tex; mode=display">s(t)=\my{p}_T(t)-\my{p}_o(t)=g(t)r(t)=\frac{g(t)}{\theta(t)}\ell.</script><p>Substituting the above equation into \eqref{eq<em>relative_motion} yields<br>\begin{align}\label{eq_st_tem}<br>\my{s}_0+\my{s}_1t+\cdots+\my{s}_nt^n+\my{h}(t)=\frac{g(t)}{\theta(t)}\ell.<br>\end{align}<br>Here, $\my{s}_0, \cdots, \my{s}_n, \ell$ are unknowns to be determined and $\my{g}(t),\theta(t),\my{h}(t)$ are known.<br>Equation~\eqref{eq_st_tem} can be reorganized to a linear equation:<br>\begin{align}\label{eq_linear_equations}<br>    \my{A}(t)\my{X} = \my{h}(t),<br>\end{align}<br>where<br>\begin{align<em>}<br>\my{X}&amp;=<br>\begin{bmatrix}<br>\my{s}_0^\mathrm{T}, \my{s}_1^\mathrm{T}, \cdots, \my{s}_n^\mathrm{T}, \ell<br>\end{bmatrix}^\mathrm{T}\in\mathbb{R}^{3n+4},<br>\end{align</em>}<br>and<br>\begin{align}\label{eq_original_A}<br>\my{A}(t)&amp;=<br>\begin{bmatrix}<br>\my{I}</em>{3\times3}, t\my{I}<em>{3\times3}, \cdots, t^n\my{I}</em>{3\times3}, \rho(t)<br>\end{bmatrix}\in\mathbb{R}^{3\times(3n+4)},<br>\end{align}<br>where<br>\begin{align}\label{eq_rho_denote}<br>\rho(t)&amp;\doteq-\dfrac{\my{g}(t)}{\theta(t)}\in\R^3.<br>\end{align}<br>Therefore, the problem that we aim to solve becomes determining whether $X$ can be uniquely solved from \eqref{eq_linear_equations}.</p><p>\subsection{Necessary and sufficient observability condition}</p><p>We next present a necessary and sufficient condition under which the solution $X$ of \eqref{eq_linear_equations} is unique.</p><p>\begin{theorem}[(Necessary and sufficient observability condition)]<br>\label{theorem<em>observability_confition}<br>The target’s motion $p_T(t)$ can be uniquely determined by the observer’s motion $p_o(t)$, the bearing $g(t)$, and the angle $\theta(t)$ if and only if<br>\begin{align*}<br>\my{h}(t)\neq\my{0}</em>{3\times1},<br>\end{align*}<br>which means that the order of the observer’s motion must be greater than the target.<br>\end{theorem}<br>\begin{proof}<br>Since the row number of $\my{A}(t)$ is less than its column number, \eqref{eq_linear_equations} is an under-determined system whose solution cannot be uniquely determined.<br>However, in the continuous time domain, we can use additional higher derivatives of this equation to uniquely determine $X$.</p><p>In particular, taking the $i$th-order derivative on both sides of \eqref{eq_linear_equations} gives $A^{(i)}(t)X=h^{(i)}(t)$. Consider any integer $N$ satisfying $N\ge n+1$. Combining the equations with $i\in{0,1,\dots,N}$  gives<br>\begin{align}\label{eq_new_linear_equtions}<br>    \bar{\my{A}}(t)\my{X} = \bar{\my{h}}(t),<br>\end{align}<br>where<br>\begin{align}\label{eq_new_A}<br>    \bar{\my{A}}(t) =<br>\left[<br>  \begin{array}{c}<br>    \my{A}(t) \<br>    \my{A}^{‘}(t) \<br>    \vdots \<br>    \my{A}^{(N)}(t)<br>  \end{array}<br>\right],\qquad<br>\bar{\my{h}}(t)\left[<br>  \begin{array}{c}<br>    \my{h}(t)\<br>    \my{h}^{‘}(t)\<br>    \vdots\<br>    \my{h}^{(N)}(t)\<br>  \end{array}<br>\right].<br>\end{align}<br>Here, $\bar{\my{A}}(t)\in\mathbb{R}^{(3N+3)\times (3n+4)}$ and $\bar{\my{h}}(t)\in\mathbb{R}^{3N+3}$.<br>Since $N\ge n+1$, $\bar{A}(t)$ is a tall matrix and \eqref{eq_new_linear_equtions} is an over-determined system.</p><p>We next examine when $\bar{A}(t)$ has full column rank.<br>Substituting \eqref{eq<em>original_A} into $\bar{A}(t)$ yields<br>\begin{align*}<br>\bar{\my{A}}(t)=<br>    \left[\begin{array}{cccc:c}<br>    \my{I}</em>{3\times3}&amp; t\my{I}<em>{3\times3}&amp; \cdots&amp; t^n\my{I}</em>{3\times3}&amp; \rho(t) \<br>    \my{0}<em>{3\times3}&amp; \my{I}</em>{3\times3}&amp; \cdots&amp; nt^{n-1}\my{I}<em>{3\times3}&amp; \rho^{‘}(t) \<br>    \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \<br>    \my{0}</em>{3\times3}&amp; \my{0}<em>{3\times3}&amp; \cdots&amp; n!\my{I}</em>{3\times3}&amp; \rho^{(n)}(t) \<br>    \hdashline<br>    \my{0}<em>{3\times3}&amp; \my{0}</em>{3\times3}&amp; \cdots&amp; \my{0}<em>{3\times3}&amp; \rho^{(n+1)}(t) \<br>\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots  \<br>\my{0}</em>{3\times3}&amp; \my{0}<em>{3\times3}&amp; \cdots&amp; \my{0}</em>{3\times3}&amp; \rho^{(N)}(t) \<br>%\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots  \<br>    \end{array}\right].<br>\end{align<em>}%\label{eq<em>expanded_tilde_A}<br>Since the top-left block of $\bar{A}(t)$ is a full-rank square matrix, $\bar{\my{A}}(t)$ has full column rank if and only if there exists $i\in{n+1,\dots,N}$ such that<br>\begin{align}\label{eq_observability_criteria_2}<br>    \rho^{(i)}(t)\neq \my{0}</em>{3\times1}.<br>\end{align}<br>Since $\rho(t)=-g(t)/\theta(t)$ as shown in \eqref{eq<em>rho_denote} and $g(t)/\theta(t)=(\my{s}_0+\my{s}_1t+\cdots+\my{s}_nt^n+\my{h}(t))/\ell$ as shown in \eqref{eq_st_tem}, we can rewrite \eqref{eq_observability_criteria_2} to<br>\begin{align}\label{eq_critia_2}<br>-\dfrac{1}{\ell}(\my{s}_0+\my{s}_1t+\cdots+\my{s}_nt^n+\my{h}(t))^{(i)}\neq \my{0}</em>{3\times1}.<br>\end{align}<br>Since $i\ge n+1$, \eqref{eq<em>critia_2} is equivalent to<br>\begin{align}\label{eq_observability_criteria_final}<br>\my{h}^{(i)} (t) \neq \my{0}</em>{3\times1}.<br>\end{align}<br>According to the definition of $\my{h}(t)$ in \eqref{eq_definition_h}, the condition in \eqref{eq_observability_criteria_final} is equivalent to<br>\begin{align</em>}<br>\my{h}(t)\neq \my{0}_{3\times1}.<br>\end{align*}<br>The proof is complete.<br>\end{proof}</p><p>Some important remarks about Theorem~\ref{theorem_observability_confition} are given below.</p><p>1) The necessary and sufficient condition suggested by Theorem~\ref{theorem_observability_confition} is that the observer should have higher-order motion than the target.<br>For example, when the target is stationary, the observer should move with a nonzero velocity. When the target moves with a constant velocity, the observer should move with a nonzero acceleration.</p><p>2) The necessary and sufficient condition given by Theorem~\ref{theorem_observability_confition} has a \emph{key difference} from the bearing-only case that the higher-order motion in the bearing-angle case is \emph{not} required to be orthogonal to the bearing vector, making the bearing-angle approach more flexible than the bearing-only one.<br>For example, the bearing-angle approach can estimate the target’s motion even if the observer simply moves along the bearing vector.</p><p>3) In the special case where the target moves with a constant velocity, the condition in Theorem~\ref{theorem_observability_confition} is consistent with the one obtained in Section~\ref{sec_analysis_of_observability_matrix}. Although the condition in Theorem~\ref{theorem_observability_confition} allows more general target motion, the analysis in Section~\ref{sec_analysis_of_observability_matrix} is still meaningful since it is directly related to the dynamic model used in the pseudo-linear Kalman filter.</p><p>4) In practice, we would not estimate the target’s motion by using the method of solving an equation like \eqref{eq_new_linear_equtions}. That is because such a method involves calculating high-order derivatives, which are challenging to obtain accurately in practice. The role of this equation is to provide a fundamental perspective on whether there is sufficient information to uniquely recover the target’s motion.</p><p>\subsection{Number of observations required}</p><p>\begin{figure<em>}[!ht]<br>\normalsize<br>\begin{align}<br>\label{eq<em>A_22}<br>\tilde{A}<br>\rightarrow<br>\left[\begin{array}{ccccc:c}<br>\my{I} &amp; t_1\my{I} &amp; \cdots &amp; t_1^{n-1}\my{I} &amp; t_1^n\my{I} &amp; \rho(t_1) \<br>\my{0} &amp; \my{I} &amp; \cdots &amp; {\Delta(t_2^{n-1}, t_1^{n-1})}\my{I} &amp; {\Delta(t_2^n, t_1^n)}{}\my{I} &amp; {\Delta(\rho(t_2),\rho(t_1) )}{} \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots \<br>\my{0} &amp; \my{0} &amp; \cdots &amp; (n-1)!\my{I} &amp; {\Delta^{n-1}(t_n^n, \cdots , t_1^n)}{} &amp; {\Delta^{n-1}(\rho(t_n),\cdots,\rho(t_1) )}{} \<br>\my{0} &amp; \my{0} &amp; \cdots &amp; \my{0} &amp; n!\my{I} &amp;  {\Delta^{n}(\rho(t</em>{n+1}),\cdots,\rho(t<em>1) )}{} \<br>\hdashline<br>\my{0} &amp; \my{0} &amp; \cdots &amp; \my{0} &amp; \my{0} &amp; {\Delta^{n+1}(\rho(t</em>{n+2}),\cdots,\rho(t_1) )}{} \<br>\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots  \<br>\my{0} &amp; \my{0} &amp; \cdots &amp; \my{0} &amp; \my{0} &amp; {\Delta^{N-1}(\rho(t_N),\cdots,\rho(t_1) )}{} \<br>\end{array}<br>\right]<br>\end{align}<br>\hrulefill<br>\vspace</em>{4pt}<br>\end{figure*}</p><p>It is of practical importance to study how many discrete observations are required to recover the target’s motion. Although Theorem~\ref{theorem_observability_confition} gives an observability condition, it does not answer this question because it is based on the continuous time domain. We next answer this question by exploring multiple discrete time steps.</p><p>\begin{theorem}[(Number of discrete observations)]\label{theorem_observation_number}<br>If the observer’s motion satisfies the observability condition in Theorem~\ref{theorem_observability_confition}, it is necessary and sufficient to use at least $n+2$ observations to recover the target’s motion. Here, $n$ is the order of the target’s polynomial motion as shown in \eqref{eq_target_nth_Order}.<br>\end{theorem}<br>\begin{proof}<br>Consider $t_1,\dots,t_N$ time instances. Each time instance corresponds to an equation like \eqref{eq_linear_equations}: $\my{A}(t_i)\my{X} = \my{h}(t_i)$ for $i=1,\dots,N$.<br>Combining these equations gives<br>\begin{align}\label{eq_convergence_linear_eqs}<br>\tilde{\my{A}}\my{X}=\tilde{\my{h}},<br>\end{align}<br>where<br>\begin{align}\label{eq_new_A_2}<br>    \tilde{\my{A}} =<br>\left[<br>  \begin{array}{c}<br>    \my{A}(t_1) \<br>    \vdots \<br>    \my{A}(t_N)<br>  \end{array}<br>\right],\qquad<br>\tilde{\my{h}}\left[<br>  \begin{array}{c}<br>    \my{h}(t_1)\<br>    \vdots\<br>    \my{h}(t_N)\<br>  \end{array}<br>\right].<br>\end{align}<br>Here, $\tilde{\my{A}}\in\mathbb{R}^{(3N) \times (3n+4)}$ and $\tilde{\my{h}}\in\mathbb{R}^{3N}$.</p><p>(\emph{Necessity}) Since $X\in\R^{3n+4}$, we need at least $N\ge n+2$ observations so that $\tilde{\my{A}}$ is a tall matrix and hence \eqref{eq_convergence_linear_eqs} is an over-determined system.</p><p>(\emph{Sufficiency})<br>Suppose we have $N\ge n+2$ discrete observations.<br>Substituting \eqref{eq<em>original_A} into \eqref{eq_new_A_2} yields<br>\begin{align*}<br>\tilde{A} =<br>\begin{bmatrix}<br>\my{I}</em>{3\times 3} &amp; t<em>1\my{I}</em>{3\times 3} &amp; \cdots &amp; t<em>1^n\my{I}</em>{3\times 3} &amp; \rho(t<em>1) \<br>\my{I}</em>{3\times 3} &amp; t<em>2\my{I}</em>{3\times 3} &amp; \cdots &amp; t<em>2^n\my{I}</em>{3\times 3} &amp; \rho(t<em>2)  \<br>\vdots &amp; \vdots &amp;&amp; \vdots &amp; \vdots  \<br>\my{I}</em>{3\times 3} &amp; t<em>{n+1}\my{I}</em>{3\times 3} &amp; \cdots &amp; t<em>{n+1}^n\my{I}</em>{3\times 3} &amp; \rho(t<em>{n+1}) \<br>\my{I}</em>{3\times 3} &amp; t<em>{n+2}\my{I}</em>{3\times 3} &amp; \cdots &amp; t<em>{n+2}^n\my{I}</em>{3\times 3} &amp; \rho(t<em>{n+2})\<br>\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots\<br>\my{I}</em>{3\times 3} &amp; t<em>{N}\my{I}</em>{3\times 3} &amp; \cdots &amp; t<em>{N}^n\my{I}</em>{3\times 3} &amp; \rho(t<em>{N})\<br>\end{bmatrix}.<br>\end{align<em>}<br>Starting from the last line in $\tilde{A}$, subtract the previous line from each subsequent line, and repeat this process.<br>Finally, we can obtain \eqref{eq_A_22} (the equation is too long and located at the top of another page).<br>Here, $\Delta^n$ represents the $n$th-order time difference \citep{MilneThomson2000}.<br>For example, $\Delta (a_2, a_1)=(a_2-a_1)/\delta t$, $\Delta^2 (a_3, a_2, a_1) = \Delta (\Delta(a_3, a_2), \Delta(a_2, a_1))=[(a_3-a_2)/\delta t-(a_2-a_1)/\delta t]/\delta t$.<br>When $\delta t$ is sufficiently small, the time difference is an approximation of the derivative.<br>When the observability condition in Theorem~\ref{theorem_observability_confition} is satisfied, there exists $i\ge n+1$ such that $\rho^{(i)}(t)\neq 0$ as shown in \eqref{eq_observability_criteria_2}. As a result, there exists $i\ge n+1$ such that<br>\begin{align</em>}<br>\Delta^{i}(\rho(t</em>{i+1}),\cdots,\rho(t_1))\neq \my{0}.<br>\end{align*}<br>The above implication is valid because $\Delta^{i}$ is an approximation of the $i$th-order derivative when $\delta t$ is sufficiently small.<br>Then, $\tilde{A}$ in \eqref{eq_A_22} has full column rank and hence \eqref{eq_convergence_linear_eqs} has a unique solution.<br>\end{proof}</p><p>Theorem~\ref{theorem_observation_number} suggests that when the target is stationary and hence $n=0$, at least two discrete observations are sufficient to localize the target. This is true even if the two observations are acquired when the observer moves along the bearing vector.<br>When the target moves with a constant velocity and hence $n=1$, at least three discrete observations are sufficient to localize the target, which is consistent with the results in Section~\ref{sec_analysis_of_observability_matrix}.</p><p>\section{Numerical Simulation Results}\label{sec_matlab_simulation}<br>\begin{figure<em>}[!t]<br>\centering<br>\subfloat[Scenario 1: Circular motion around the target. Both the bearing-only and bearing-angle approaches work well, but the bearing-angle one converges faster.]{<br>\includegraphics[width=1 \linewidth]{fig_matlab_1}<br>\label{fig_matlab_1}<br>}<br>\hfill<br>\subfloat[Scenario 2: Straight motion towards and backwards the target. The bearing-only approach fails, but the bearing-angle approach works effectively.]{<br>\includegraphics[width=1 \linewidth]{fig_matlab_2}<br>\label{fig_matlab_2}<br>}<br>\hfill<br>\subfloat[<br>Scenario 3: Approaching the target by a guidance law. The bearing-only approach works unstably, but the bearing-angle approach works effectively.]{<br>\includegraphics[width=1 \linewidth]{fig_matlab_3}<br>\label{fig_matlab_3}<br>}<br>\caption{Numerical simulation results based on 100 Monte Carlo runs in three scenarios.}<br>\end{figure</em>}</p><p>\begin{figure<em>}[!t]<br>\label{fig_matlab_varying_ell}<br>\centering<br>\subfloat[<br>The observer moves around the square-shaped target. The target spins rapidly at $2\pi$~rad/s.]{<br>\includegraphics[width=1 \linewidth]{fig_matlab_4}<br>\label{fig_matlab_4}<br>}<br>\hfill<br>\subfloat[<br>The observer moves along the bearing vector. The target’s spinning speed is $\pi/8$~rad/s.]{<br>\includegraphics[width=1 \linewidth]{fig_matlab_pi_8}<br>\label{fig_matlab_pi_8}<br>}<br>\caption{Numerical simulation results for time-varying $\ell$.}<br>\end{figure</em>}<br>This section presents a set of numerical simulation results to demonstrate the effectiveness of the proposed bearing-angle approach.</p><p>The values of the parameters in two estimators are selected as $\sigma<em>v=10^{-3}$, $\sigma_l=10^{-4}$, $\sigma</em>\mu=0.01$, and $\sigma_w=0.01$.<br>The selection of these values is inspired by the measurement noises obtained in the AirSim simulation and real-world experiments as shown later.<br>The initial covariance matrix of the estimated states is set to $P(t_0)=0.1I$.<br>The target is a circle whose diameter is $\ell=1$.<br>The update rate of the system is $50$~Hz.<br>In addition, we use the same parameter values across all the simulation examples to verify the robustness of the algorithm.<br>Better performances can be achieved if the parameters are well-tuned for specific scenarios.<br>We perform $N_x=100$ Monte Carlo simulations for each scenario.</p><p>We use the normalized-estimation error squared (NEES) \citep{bar1998estimation} to analyze the consistency of the estimation algorithms.<br>In particular, the value of the average NEES is</p><p>\begin{align}<br>    \bar{\epsilon}<em>{\text{NEES}}=\dfrac{1}{N_x}\sum</em>{i=1}^{N_x}(x-\hat{x}_i)^\mathrm{T}P_i^{-1}(x-\hat{x}_i),<br>    \label{eq_nees}<br>\end{align}<br>%where $n_x\in\mathbb{R}$ is the dimension of the estimated states ($n_x=6$ for bearing-only, and $n_x=7$ for bearing-angle).<br>where $\hat{x}_i$ is the estimated states in the $i$th simulation, and $P_i$ is the covariance matrix obtained from the estimator in the $i$th simulation.<br>%The expectation of the average NEES value would be equal to the number of the estimated states.</p><p>Finally, image acquisition and visual detection are not considered in these numerical simulation scenarios. They will be considered in Section~\ref{sec_airsim_simulation} and Section~\ref{sec_real_world_experimental_validation}.</p><p>\subsection{Scenario 1: Circular motion around the target}<br>In the first scenario, the target is stationary and located at $\my{p}_T=[0, 10]^\mathrm{T}$.<br>The observer moves on a circle centered at the target with the speed of $3$~m/s (see Fig.~\ref{fig_matlab_1}).<br>The radius of the circle is $5$~m.<br>The initial estimates are $\hat{p}_o(t_0) = [0, 13]^\mathrm{T}$, $\hat{v_o}(t_0)=[0, 0]^\mathrm{T}$, $\hat{\ell}(t_0)=1.6$.<br>During this process, the bearing vector varies while the angle subtended by the target remains constant.<br>The angle measurement varies slightly due to the measurement noise.<br>This scenario is favorable to the conventional bearing-only approach because its observability condition that the target should be viewed from different angles is well satisfied \citep{Li2022}.</p><p>Fig.~\ref{fig_matlab_1} shows the estimation results by the two approaches of bearing-only and bearing-angle.<br>As can be seen, both algorithms perform well.<br>The convergence of the bearing-angle approach is faster than the bearing-only one, as shown in the middle and right subfigures of Fig.~\ref{fig_matlab_1}, due to the additional angle measurement.<br>The bearing-angle approach can successfully estimate the size of the target as shown in the right subfigure of Fig.~\ref{fig_matlab_1}.</p><p>\subsection{Scenario 2: Straight motion towards and backwards the target repeatedly}<br>In the second scenario, the target is also stationary but the observer moves along a straight line towards and backwards the target repeatedly (Fig.~\ref{fig_matlab_2}).<br>During this process, the bearing vector remains constant while the angle varies.<br>This scenario is most challenging for the bearing-only approach because its observability condition is not fulfilled.</p><p>In this simulation scenario, the target is stationary and located at $\my{p}_T(t_0)=[0, 10]^\mathrm{T}$.<br>The observer moves along a straight line towards and backwards the target with a constant acceleration of $-2$~$\text{m/s}^2$. The initial conditions are $v_o(t_0)=[0, 4]^\mathrm{T}$ and $\my{p}_o (t_0)= [0,5]^\mathrm{T}$.<br>The initial estimates are $\hat{p}_o(t_0) = [0, 8]^\mathrm{T}$, $\hat{v_o}(t_0)=[0, 0]^\mathrm{T}$, $\hat{\ell}(t_0)=0.8$.<br>In this scenario, the true bearing of the target relative to the observer remains unchanged though the bearing measurement may vary slightly due to the measurement noise.</p><p>Fig.~\ref{fig_matlab_2} shows the estimation results of the bearing-only and bearing-angle approaches.<br>As can be seen, the bearing-only approach diverges since its observability condition is not satisfied.<br>By contrast, the proposed bearing-angle approach converges, and is able to localize the target and estimate its size, which demonstrates the strong observability of the bearing-angle approach.<br>One may notice that the estimated size and the NEES value get worse first before converging.<br>This is because the noise level of the angle is set to be constant. Since the angle is small in the beginning, the noise-angle ratio is large, causing a relatively large NEES value.</p><p>\begin{figure<em>}[!t]<br>\centering<br>\subfloat[An AirSim simulation experimental scenario.]{<br>\includegraphics[width=0.5 \linewidth]{fig_architecture_airsim}<br>\label{fig_architecture_airsim}<br>}<br>\subfloat[Samples of the dataset collected automatically in AirSim.]{<br>\includegraphics[width=0.5 \linewidth]{fig_airsim_dataset}<br>\label{fig_airsim_dataset}<br>}<br>\caption{The setup of the AirSim simulation experiments.}<br>\end{figure</em>}</p><p>\begin{figure<em>}[!t]<br>    \centering<br>    \includegraphics[width=1\linewidth]{fig_box_airsim}<br>    \caption{The software architecture of the AirSim simulation system. AirSim is a plugin for Unreal Engine. Three programmed modules (Offline training, Online estimation, and MAV control) communicate with the AirSim plugin through APIs.}<br>    \label{fig_box_airsim}<br>\end{figure</em>}</p><p>\subsection{Scenario 3: Approaching the target by a guidance law}<br>The third scenario is more complex than the first two. Here, the target moves with a constant velocity where the observer is controlled by a proportional navigation guidance (PNG) law to approach the target (Fig.~\ref{fig_matlab_3}).<br>During this process, both the bearing and angle vary.<br>This scenario is also challenging for the bearing-only approach because its observability is weak due to the fact that the lateral motion of the observer is small.<br>Many researchers have studied how to add extra control commands to the PNG to enhance the observability based on the bearing-only approach \citep{Song1996, Seo2015, Lee2015}.</p><p>In this simulation scenario, the target moves along a straight line with a constant velocity $\my{v}_T=[1/\sqrt{2}, 1/\sqrt{2}]^\mathrm{T}$.<br>The observer’s velocity magnitude is constantly $3$~m/s while the velocity direction is controlled by a PNG law.<br>The navigation gain of the PNG law is selected as one.<br>The initial estimates of the target’s states are the same as Scenario~1.<br>The simulation stops just before the observer collides with the target.</p><p>Fig.~\ref{fig_matlab_3} shows the estimation results by the bearing-only and bearing-angle approaches. As can be seen, the bearing-angle algorithm successfully converges before the collision occurs, but the bearing-only algorithm fails to estimate the target’s states due to its weak observability.<br>This simulation example demonstrates that the bearing-angle algorithm can be used directly in the guidance scenario without extra maneuvers required by the bearing-only approach \citep{Song1996, Seo2015, Lee2015}.</p><p>\subsection{Simulation results for time-varying $\ell$}<br>\label{sec_matlab_varying_ell}<br>Although $\ell$ is assumed to be invariant, it is meaningful to challenge the proposed bearing-angle approach by considering time-varying $\ell$.<br>We will see through simulation examples that the bearing-angle approach is still effective when $\ell$ varies slowly. It becomes unstable when $\ell$ varies rapidly since the assumption of invariant $\ell$ is severely invalid.</p><p>Suppose that the target object has a square shape. Then, $\ell$ varies when the object is observed from different viewing angles or the object spins.<br>Fig.~\ref{fig_matlab_4} shows a scenario where the observer moves around the target, whose spinning speed is $2\pi$~rad/s.<br>The red curve in the right subfigure represents the true value of $\ell$, which varies rapidly.<br>As can be seen, the bearing-angle algorithm works effectively though there is a small estimation bias.<br>Fig.~\ref{fig_matlab_pi_8} shows a scenario where the observer moves along the bearing vector. The spinning speed of the target object is $\pi/8$~rad/s.<br>As can be seen, the bearing-only approach diverges due to the lack of observability. The bearing-angle algorithm can still converge since $\ell$ varies slowly.<br>When we further increase the spinning speed of the target, the bearing-angle algorithm will also diverge because the algorithm cannot distinguish whether the change of $\theta$ is caused by the change of $\ell$ or the change of $r$.</p><p>\section{AirSim Simulation Results}\label{sec_airsim_simulation}<br>In this section, we show simulation results under a more realistic setup. In particular, the simulation is based on AirSim, a simulator that can provide high-quality visual simulation \citep{Shah2017}. Nonlinear MAV dynamics and control are also considered.</p><p>\begin{figure<em>}[!t]<br>\centering<br>\subfloat[The target MAV hovers stationarily, while the observer MAV approaches the target MAV under the control of \eqref{eq_tracking_control}.]{<br>\includegraphics[width=1 \linewidth]{fig_airsim_1}<br>\label{fig_airsim_1}<br>}<br>\hfill<br>\subfloat[The target MAV moves with a constant velocity, while the observer MAV follows the target MAV under the control of \eqref{eq_tracking_control}.]{<br>\includegraphics[width=1 \linewidth]{fig_airsim_2}<br>\label{fig_airsim_2}<br>}<br>\caption{AirSim simulation results in the approaching and following scenarios.}<br>\label{fig_airsim}<br>\end{figure</em>}</p><p>\subsection{Simulation setup}</p><p>\begin{figure<em>}[!t]<br>\centering<br>\subfloat[Estimation results when $\sigma_l=10^{-4}$ and the other parameters are the same as those in Section~\ref{sec_sim_res_for_tracking}.]{<br>\includegraphics[width=1 \linewidth]{fig_airsim_6_1}<br>\label{fig_airsim_6_1}<br>}<br>\hfill<br>\subfloat[Estimation results when $\sigma_l=0.01$ and the other parameters are the same as those in Section~\ref{sec_sim_res_for_tracking}.]{<br>\includegraphics[width=1 \linewidth]{fig_airsim_6_2}<br>\label{fig_airsim_6_2}<br>}<br>\caption{AirSim simulation results in the circular motion scenario where $\ell$ varies.}<br>\label{fig_airsim_6}<br>\end{figure</em>}</p><p>Fig.~\ref{fig_architecture_airsim} shows an AirSim simulation scenario.<br>As can be seen, there are two flying quadcopter MAVs. The observer MAV can capture images of the target MAV using its simulated onboard camera.<br>A simple gimbal camera controller is implemented so that the target MAV is always located inside the field of view of the camera.<br>The visual environment used in the simulation is called Landscape Mountains, which includes realistic mountains, lakes, trees, and roads. Other environments can also be used if needed.</p><p>The bearing and angle measurements are obtained from the bounding boxes generated by a Yolo-based detection algorithm.<br>A tiny-YOLO v4 network \citep{Bochkovskiy2020} is trained to detect the target MAV in the images. Although the visual detector can be replaced by other state-of-the-art ones, the tiny-YOLO v4 network is already sufficient to verify our proposed approach.<br>The architecture of the entire simulation system is shown in Fig.~\ref{fig_box_airsim}.<br>The system consists of the modules of automatic image dataset collection, Yolo-based target detection, gimbal camera control, nonlinear quadcopter dynamics, and quadcopter flight control.<br>The quadcopter dynamics and flight control used in the simulation are similar to \citep{Meier2011, Shah2017} and omitted here due to space limitation.<br>The quadcopter’s physical size varies slightly when viewed from different directions, although it is assumed to be invariant.<br>All of these factors make the Airsim simulation more realistic and challenging.</p><p>\subsection{Automatic dataset collection}<br>To train the Yolo-based detector, we developed a module to automatically collect an image dataset.<br>This module has some advantages.<br>First, it is efficient. More than ten thousand labeled images can be collected automatically in 24 hours.<br>Second, it is flexible.<br>It can acquire images with random target’s positions, random target’s attitudes, random camera’s view angles, and random background scenes.<br>These images are beneficial to achieve a good generalization ability of the detector.<br>Third, the image labels are of high quality. Since the ground truth of the target’s image is known in the simulation, the generated bounding box is tight.<br>The collected dataset contains 17,000 labeled images (see Fig.~\ref{fig_airsim_dataset}).<br>The resolution of the images is $1536\times 864$ pixels.<br>The simulation system was deployed on a Dell Precision 7920 Tower Workstation with two NVIDIA Quadro GV100 graphic cards.<br>Since the dataset is sufficient and high-quality, the detection can achieve the accuracy of mAP=99.5\%.</p><p>\subsection{Scenario 1: Approaching and following the target}<br>\label{sec_sim_res_for_tracking}<br>We first consider the scenarios where the observer MAV approaches or follows a target MAV.<br>These scenarios widely exist in practical applications such as aerial target pursuit.</p><p>We show two simulation examples in Fig.~\ref{fig_airsim_1} and Fig.~\ref{fig_airsim_2}, respectively.<br>In both examples, the observer is controlled by a controller so that it can approach the target and maintain a desired separation. In particular, the controller is<br>\begin{align}<br>\label{eq_tracking_control}<br>v_o^\text{cmd}(t)&amp;=v_T(t)+k^\text{track}\dfrac{r^2(t)-r_d^2}{r^2(t)}g(t),<br>\end{align}<br>where $v_o^\text{cmd}(t)$ is the velocity command of the observer MAV, $k^\text{track}=3$ is the control gain, and $r_d=3$ is the desired separation.<br>The magnitude of the observer’s velocity is bounded from above by $3$~m/s.<br>It should be noted that \eqref{eq_tracking_control} relies on the true position and velocity of the target MAV in the simulation. Therefore, the data is collected first and then processed offline so that we can compare the performances of the bearing-only and bearing-angle approaches.</p><p>In the first example, the target MAV hovers constantly at $p_T(t_0)=[0, 10, 10]^\mathrm{T}$.<br>The observer MAV moves along a straight line toward the target with a decreasing velocity command.<br>Since the bearing of the target MAV remains the same, this example is challenging for the bearing-only approach.<br>As shown in Fig.~\ref{fig_airsim_1}, the bearing-only approach fails to converge while the bearing-angle approach can successfully estimate the target’s motion.</p><p>In the second example, the target MAV moves with a constant velocity of $v_T=[1/\sqrt{2}, 1/\sqrt{2}, 0]^\mathrm{T}$.<br>The trajectory of the observer MAV under the control of \eqref{eq_tracking_control} is still close to (though not strictly) a straight line. As a result, the observability is weak by the bearing-only approach.<br>As shown in Fig.~\ref{fig_airsim_2}, the bearing-angle approach successfully converges while the bearing-only one fails.<br>It is notable that $\ell$ is invariant in the first example and varies slowly in the second example.</p><p>It is worth mentioning that the detection results used in the estimation algorithms are obtained from the Yolo-based estimator.<br>The ground truth obtained from AirSim is only used to calculate the errors of measurements, as shown in the right figures of Figs.~\ref{fig_airsim_1} and \ref{fig_airsim_2}.<br>It is not surprising that the measurement noises are not strictly Gaussian since the 2D bounding box is generated by a deep learning vision algorithm. It is noticed that the noises are inversely correlated to the observer-target range.<br>This is reasonable because, when the target is close to the camera and hence its image is large, the center point and the size of the bounding box usually vary for a few pixels.</p><p>The NEES values are also shown in Fig.~\ref{fig_airsim}.<br>As can be seen, the NEES value of the bearing-only approach diverges. The NEES value of the bearing-angle approach oscillates and converges slowly. The reasons are analyzed as follows. Compared to the Matlab-based numerical simulation, the visual measurements here are generated by deep learning algorithms, and the measurement noises are non-Gaussian. The non-Gaussian noises propagate into $P$ in \eqref{eq_nees} since the calculation of $P$ relies on noisy measurements. The noises may also cause an estimation bias that can further aggravate the NEES error. Moreover, although the system is observable in the two simulation examples, the observability is relatively weak compared to the case where the observer moves surrounding the target. As a result, the matrix $P$ may not be able to perfectly describe the estimation accuracy. These elements may jointly cause the convergence behavior of the NEES values shown in Fig.~\ref{fig_airsim}.</p><p>\subsection{Scenario 2: Circular motion and varying $\ell$}<br>\label{sec_sim_res_circular_scenario}</p><p>We next examine a case where $\ell$ is time-varying.<br>In particular, suppose a target quadcopter MAV hovers constantly at $p_T(t_0)=[0, 10, 10]^\mathrm{T}$.<br>The observer MAV moves on a circle centered at the target (Fig.~\ref{fig_airsim_6_1}).<br>Since the target quadcopter MAV has a square shape from the top view, its size $\ell$ is time-varying when viewed from side angles (see the red curves in the middle subfigure of Fig.~\ref{fig_airsim_6_1}).</p><p>We show two simulation examples in Fig.~\ref{fig<em>airsim_6_1} and Fig.~\ref{fig_airsim_6_2}, respectively.<br>The two simulation examples share the same measurement data but different values of $\sigma</em>\ell$.<br>Moreover, the other parameters are the same as those in Section~\ref{sec_sim_res_for_tracking}.</p><p>\begin{figure<em>}[!t]<br>\centering<br>\subfloat[Experimental setup]{<br>\includegraphics[width=0.48 \linewidth]{fig_architecture_indoor}<br>\label{fig_architecture_indoor}<br>}<br>\subfloat[Samples in the dataset]{<br>\includegraphics[width=0.48 \linewidth]{fig_car_dataset1}<br>\label{fig_car_dataset}<br>}<br>\caption{The setup of the experiments based on a hand-held camera.}<br>\end{figure</em>}</p><p>In the first simulation example, $\sigma<em>\ell$ is set to be a small value: $\sigma</em>\ell=10^{-4}$.<br>Its interpretation is that $\ell$ is treated as invariant during the process.<br>In this case, the performance of the bearing-angle approach is almost the same as the bearing-only one as shown in Fig.~\ref{fig_airsim_6_1}.<br>Since $\ell$ is treated to be invariant, the estimated value $\hat{\ell}$ converges to a constant which is the mean value of the time-varying $\ell$.</p><p>In the second simulation example, the value of $\sigma<em>\ell$ is larger than the first example: $\sigma</em>\ell = 0.01$.<br>Its interpretation is that $\ell$ is believed to be time-varying during the process.<br>In this case, the performance of the bearing-angle approach is still almost the same as the bearing-only one.<br>Moreover, since $\sigma_\ell$ is large, the bearing-angle approach can successfully estimate the true time-varying value of $\ell$.</p><p>In summary, in the case where $\ell$ varies slowly, the bearing-angle approach would degenerate to the bearing-only one.<br>The fundamental reason is that the extra information embedded in the angle measurement is used to estimate the time-varying $\ell$ rather than improving the observability of the target’s motion.</p><p>\begin{figure<em>}[!t]<br>\centering<br>\subfloat[Case 1: The observer moves around the target. Both the bearing-only and bearing-angle approaches work well.]{<br>\includegraphics[width=1 \linewidth]{fig_indoor_9}<br>\label{fig_indoor_9}<br>}<br>\hfill<br>\subfloat[Case 2: The observer moves close or far from the target periodically. The bearing-angle approach performs effectively, but the bearing-only approach works unstably.]{<br>\includegraphics[width=1 \linewidth]{fig_indoor_6}<br>\label{fig_indoor_6}<br>}<br>\caption{Experimental results based on a hand-held camera.}<br>\end{figure</em>}</p><p>\section{Real-World Experimental Results}\label{sec_real_world_experimental_validation}</p><p>In this section, two sets of real-world experiments are presented to further verify the effectiveness of the approach. The first is based on a hand-held camera and a ground robot.<br>The second is based on two quadcopter MAVs. The second experimental scenario is motivated by aerial target pursuit tasks.</p><p>\subsection{Experiment 1: Hand-held camera}</p><p>The experimental setup is shown in Fig.~\ref{fig_architecture_indoor}.<br>The observer is a hand-held camera (Hik Vision DS-E14S) connected to a laptop. The camera’s intrinsic parameters are calibrated beforehand.<br>The robot built on Mecanum wheels can move in any direction on the ground under velocity control.<br>The ground truth of the states of the camera and the robot are provided by a Vicon indoor motion capture system.<br>The key experimental specifications are listed in Table~\ref{table_indoor_hardware}.</p><p>A dataset of 5,514 images was collected and used to train a tiny-YOLO v4 network to detect the target robot (see Fig.~\ref{fig_car_dataset}).<br>The detection precision of the trained network is mAP=99.8\%.<br>In the experiment, the target robot is commanded to move with a constant velocity.<br>In the meantime, a person holding the camera moves along some trajectories.<br>Two different cases are studied. In both of the cases, the target robot moves with a constant velocity $v_T=[-0.1, 0.1 ,0]^\mathrm{T}$.<br>The noises of the measurements are calculated based on the ground truth provided by the Vicon system. The noises are shown in the right subfigures of Fig.~\ref{fig_indoor_9} and Fig.~\ref{fig_indoor_6}.</p><p>In the first case, the camera is held about 1.5 meters above the ground and moves around the target robot. In this case, the bearing vector varies sufficiently and hence the observability conditions for the bearing-only and bearing-angle approaches are both well satisfied. As shown in Fig.~\ref{fig_indoor_9}, both approaches perform well in this case while the bearing-angle approach performs slightly better than the bearing-only one.</p><p>In the second case, the camera moves along the trajectory of the robot by getting close or far from it periodically.<br>In this case, the angle varies significantly, but the bearing does not.<br>Without surprise, the bearing-only approach performs poorly in this case due to weak observability  (Fig.~\ref{fig_indoor_6}).<br>By contrast, the bearing-angle approach can perform stably due to its enhanced observability.<br>\begin{table}<br>\begin{center}<br>\caption{Key specifications of the \emph{indoor} hardware system.}<br>\label{table_indoor_hardware}<br>\begin{tabular}{l|lll}<br>\hline<br> &amp; Parameter &amp; Value &amp; Unit \<br>\hline<br>\multirow{2}<em>{Camera} &amp; Resolution &amp; 640$\times$ 480 &amp; pixel\<br>~ &amp; Max frequency &amp; 30 &amp;fps\<br>\hline<br>\multirow{2}</em>{Robot} &amp; Max speed &amp; 1&amp; m/s \<br>~ &amp; Diameter size &amp; 295 &amp; mm\<br>\hline<br>\multirow{2}*{Vicon} &amp; Localization accuracy &amp; 1 &amp; mm \<br>~ &amp; Max frequency &amp; 100 &amp; Hz\<br>\hline<br>\end{tabular}<br>\end{center}<br>\end{table}</p><p>\subsection{Experiment 2: MAV-following-MAV}<br>\label{sec_mav_following_mav}</p><p>\begin{table}<br>\begin{center}<br>\caption{Key specifications of the \emph{outdoor} hardware system.}<br>\label{table_M300}<br>\begin{tabular}{c|lll}<br>\hline<br> &amp; Parameter &amp; Value &amp; Unit \<br>\hline<br>\multirow{5}{<em>}{\makecell[c]{M300 \quadcopter}} &amp; Diagonal size &amp; 895 &amp; mm\<br>~&amp;Total mass &amp; 7.4 &amp; kg \<br>~&amp;Max pitch/roll &amp; 30 &amp; degree \<br>~&amp;Max flight time &amp; 30 &amp; minutes\<br>\hline<br>\multirow{2}{</em>}{RTK} &amp; Accuracy &amp; 1 &amp; cm \<br>~&amp; Max frequency &amp; 10 &amp; Hz\<br>\hline<br>\multirow{3}{*}{\makecell[c]{H20 \ gimbal \&amp; \ camera} } &amp; Resolution &amp;1920$\times$1080  &amp; pixel\<br>~&amp; Frequency &amp; 15 &amp; Hz \<br>~ &amp; Max angular rate &amp; 180 &amp; deg/s\<br>\hline<br>\end{tabular}<br>\end{center}<br>\end{table}</p><p>\begin{figure<em>}[!t]<br>\centering<br>\subfloat[Hardware platforms]{<br>\includegraphics[width=0.48 \linewidth]{fig_M300}<br>\label{fig_M300}<br>}<br>\subfloat[Samples of the images in the dataset]{<br>\includegraphics[width=0.48\linewidth]{fig_M300_dataset1}<br>\label{fig_M300_dataset}<br>}<br>\hfill<br>\subfloat[System architecture]{<br>\includegraphics[width=0.48 \linewidth]{fig_outdoor_hardware}<br>\label{fig_outdoor_hardware}<br>}<br>\caption{The setup of the MAV-following-MAV experiment.}<br>\end{figure</em>}</p><p>\begin{figure<em>}[!t]<br>    \centering<br>    \includegraphics[width=1\linewidth]{fig_outdoor_1}<br>    \caption{The results of the MAV-following-MAV experiments. The bearing-angle approach performs effectively, but the bearing-only approach works unstably.}<br>    \label{fig_outdoor_1}<br>\end{figure</em>}</p><p>Two MAV platforms were developed based on DJI M300 quadcopters (Fig.~\ref{fig_M300}).<br>The MAV platforms are equipped with RTK GPS modules for accurate self-localization, an H20 camera for visual detection, a Manifold 2G onboard computer for onboard flight control, and a Zigbee module for wireless communication.<br>Some key specifications of the MAV platforms are listed in Table~\ref{table_M300}.<br>The structure of the hardware perception and communication system is illustrated in Fig.~\ref{fig_outdoor_hardware}.<br>The target MAV is also equipped with an RTK GPS module, whose measurements are used as the ground truth to calculate the noises of the visual measurements. The noises are shown in the right subfigure of Fig.~\ref{fig_outdoor_1}.</p><p>The experiment consists of two stages: data acquisition and offline data processing.<br>In the data acquisition stage, the target MAV is commanded to fly with a constant velocity, and the observer MAV is automatically controlled to follow the target MAV to maintain a constant distance from the target.<br>More specifically, the procedure of the flight experiment is as follows. Initially, the observer MAV is placed about 20 meters behind the target MAV on the ground. Then, the two MAVs take off and fly to the same specified height automatically upon a takeoff command sent from the ground control station.<br>After they have reached the desired height, all deployed algorithms are activated.<br>Then, the target MAV moves with a constant velocity of $v_T=[1/\sqrt{2}, 1/\sqrt{2}, 0]^\mathrm{T}$. The observer MAV approaches the target by the controller in \eqref{eq_tracking_control}. It takes the observer MAV about eight seconds to reach the desired relative distance.<br>Then, the two MAVs fly with the same velocity and remain relatively stationary for another 20 seconds.<br>Finally, the ground station sends a stop command and the two MAVs return and land automatically.</p><p>During the flight, the gimbal camera of the observer MAV is automatically controlled so that the target MAV is maintained in the field of view.<br>It is noted that the control of the gimbal camera and the observer MAV is not based on the visual detection results. Instead, the control is based on the measurements provided by the RKT GPS and inter-MAV wireless communication. In this way, we can analyze the image and flight data offline and compare the performance of the two approaches of bearing-angle and bearing-only.<br>The acquired images and flight data are saved on the onboard computer during the flight. A dataset of 5,341 images was collected (Fig.~\ref{fig_M300_dataset}) and used to train a tiny-YOLO v4 network.<br>The detection precision of the trained network reaches mAP=99.8\%.</p><p>The experimental results are shown in Fig.~\ref{fig_outdoor_1}.<br>As can be seen, the bearing-angle approach performs well. By contrast, the bearing-only approach only works well before the observer MAV reached the desired position relative to the target MAV. That is because the bearing varies significantly during this process due to the fluctuation of the observer’s motion caused by the flight control. However, the bearing-only approach diverges quickly thereafter when the bearing stops varying significantly.</p><p>\section{Conclusion}\label{section_conclusion}<br>Motivated by the limitation of the existing bearing-only approach, this paper proposed and analyzed a novel bearing-angle approach for vision-based target motion estimation. We showed that the observability by the bearing-angle approach is significantly enhanced compared to the bearing-only one.<br>As a result, the requirement of the observer’s extra motion for observability enhancement can be significantly relaxed.<br>As we showed in various experiments, the bearing-angle approach can successfully estimate the target’s motion in many scenarios where the bearing-only approach fails.<br>The enhanced observability of the bearing-angle approach comes with no additional cost since almost all vision detection algorithms can generate bounding boxes.<br>One assumption adopted in the bearing-angle approach is that the target’s physical size is invariant to different viewing angles. Although this assumption is approximately valid in many tasks as demonstrated in this paper, it is meaningful to study how to relax or remove this assumption in the future.</p><p>%\section*{Acknowledgements}<br>%The authors would like to thank the anonymous reviewers and the editors for their helpful advice for improving this work.</p><p>\section*{Declaration of conflicting interests}<br>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p><p>\section*{Funding}<br>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this artical: This work was supported by the Hangzhou Key Technology Research and Development Program (Grant 20212013B09), and the Research Center for Industries of the Future at Westlake University (Grant WU2022C027).</p><p>\bibliographystyle{SageH}<br>%\bibliographystyle{SageV}<br>\bibliography{paper}</p><p>\end{document}</p>]]></content>
    
    
    <summary type="html">🧵本文研究了使用移动单目相机估计移动目标物体运动的问题</summary>
    
    
    
    <category term="阅读" scheme="https://www.adunas.top/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="视觉导航" scheme="https://www.adunas.top/tags/%E8%A7%86%E8%A7%89%E5%AF%BC%E8%88%AA/"/>
    
  </entry>
  
  <entry>
    <title>关于爱莉西亚局长的个人回忆</title>
    <link href="https://www.adunas.top/posts/20240222b.html"/>
    <id>https://www.adunas.top/posts/20240222b.html</id>
    <published>2024-02-22T11:12:03.000Z</published>
    <updated>2024-02-22T13:46:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文学导航"><a href="#文学导航" class="headerlink" title="文学导航"></a><a href="./20240224d.html#关于爱莉西亚局长的个人回忆">文学导航</a></h1><blockquote><p>警察局档案编号：2024-01-11-001</p><p>档案名称：关于爱莉西亚局长的个人回忆</p><p>档案作者：英仙座，警察局副局长</p></blockquote><p>&emsp;&emsp;我与爱莉西亚局长的相识，要追溯到三年前的一起奇怪的案件。当时，我还是一个银行劫匪，和我的同伙一起策划了一次大规模的抢劫。我们打算在杰拉德·弗雷泽的银行里装上炸弹，然后趁着混乱，抢走所有的钱。我们以为我们的计划十分完美，没想到，遇到了爱莉西亚，也因此认识了银行家。</p><p>&emsp;&emsp;她当时还不是局长，只是一个刚刚调来的刑警，负责调查一起涉及黑社会的枪击案。她恰巧在那家银行里，发现了我们的炸弹，就立刻通知了警方，然后冲进了我们的藏身处，一个接一个地把我们制服了。她的动作十分迅速，几乎没有给我们反应的机会。我是最后一个被她抓住的，她用枪指着我的头，说：“你们这些无耻的家伙，竟敢威胁无辜的人民，应该受到惩罚。”她的眼神十分冷酷，让我感到一阵寒意。</p><p>&emsp;&emsp;但是，就在她准备扣动扳机的时候，她突然停住了，她的眼神变得柔和，她说：“不过，你还有救赎的机会，你可以选择跟我走，或者留在这里等死。”我不知道她为什么会突然改变主意，也不知道她要带我去哪里，但是我觉得我没有别的选择，就跟着她走了。</p><p>&emsp;&emsp;从那以后，我就成了爱莉西亚局长的得力助手，也是她唯一的朋友。她对我很好，总是给我讲一些有趣的故事，还教我一些奇怪的知识。她说她是从一个叫做 “逐火之蛾” 的组织来的，那里有很多奇妙的事物，还有一些她的战友。</p><p>&emsp;&emsp;我不知道她说的这些是真是假，但我觉得她很可信，也很有魅力。她在工作上很有能力，能够迅速解决各种棘手的案件，还能和各方打好关系。她在生活上很有趣，经常拉着我去逛街，让我帮她挑选衣服和化妆品，有时候我心不在焉，她又会拉着我的手对我说：“你快对我说，艾莉西亚穿什么都好看，好不好嘛？”有时候她也会拉着我去看恐怖电影，演到吓人的场景时她也会害怕的缩成一团，和之前勇敢无畏的局长判若两人。虽然我也很害怕，但还是会摸摸她的头，对她说：“我会永远保护你的。”而她会趁机靠在我的肩膀上说：“你可真是我的小英雄！”</p><p><img src="https://picture.adunas.top/Article/ElysiaA.png" alt=""></p><p>&emsp;&emsp;有一次我们解决了哈山的案子，中间和她分开了一段时间，结果听到她的一声惨叫，但是当我过来之后她只是一脸笑意的看着我，手上拿着一个奇怪的怀表。之后的日子里她有时候会突然消失一段时间，然后又突然出现在我的面前，她从不告诉我她去了哪里，做了什么，只是笑着说：“你不用担心，我只是有些私事要办。”我总是觉得她在隐瞒什么，但我不敢多问，只是默默地等待她的归来，只觉得她有一种神奇的力量。</p><p>&emsp;&emsp;她经常让我陪她去一些危险的地方，说是为了调查一些案件，其实是为了寻找她的秘密。她总是让我遇到各种麻烦，比如被黑社会追杀，被邪恶的力量感染，等等。她每次都会在最后一刻出现，救我一命，然后对我说：“你真是个笨蛋，怎么总是让自己陷入危险，你不知道我有多担心你吗？”她的语气总是带着一种嘲讽，让我觉得她是在故意捉弄我，但是她的眼神却又充满了关切，让我觉得她是在真心保护我。我不知道她到底是怎么想的，但是我总是感激她，也总是原谅她。</p><p>&emsp;&emsp;直到那一天，一切都改变了。那是一天晚上，我们在现场发现了一个小女孩，爱莉西亚看到她，就惊呼了一声，说：“是你！”然后，她就冲上去，抱住了那个女孩。接下来的事情，我就记不太清楚了。我只记得爱莉西亚局长和那个女孩说了一些话，然后她就对我说：“对不起，我要走了，我有很重要的事情要做，和她有关。”我问她要去哪里，她说：“去一个你无法跟随的地方，去完成我的使命。”而我无法抛下她不管，还是偷偷跟了过去。</p><p>&emsp;&emsp;中间撕掉了很多页（）</p><p>&emsp;&emsp;那一天的场景仍然无数次的出现在我的梦里，我仿佛置身充满了恐怖和绝望的地方，让人只能感受到疯狂和死亡。那里有无数的黑暗，无数的怪物，无数的尖叫，无数的血肉。那里没有光明，没有生命，没有希望，没有意义，没有规则。那里只有一个无尽的混沌，一个无边的疯狂，一个无名的恐惧，一个无法的崩坏。</p><p>&emsp;&emsp;我只记得最后，艾莉西亚对我说：“傻瓜，你还是跟过来了呀，我的小英雄。”</p><details class="folding-tag" orange><summary> 点击查看语录 </summary>              <div class='content'>              <p><span class='p green'&gt;></span> 粉色头发的迷人女孩是谁呀，哦，原来是我呀！&lt;span class='p green'><</span></p><div class="audio"><audio controls preload><source src='https://picture.adunas.top/Audio/%E7%88%B1%E8%8E%89%E5%B8%8C%E9%9B%85-%E8%AF%B4%E8%B5%B7%E7%B2%89%E8%89%B2%E5%A4%B4%E5%8F%91%E7%9A%84%E5%8F%AF%E7%88%B1%E5%A5%B3%E5%AD%A9%EF%BC%8C%E4%BD%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%BC%9A%E6%83%B3%E5%88%B0%E8%B0%81%EF%BC%9F%E4%B8%89%E4%BA%8C%E4%B8%80%E5%9B%9E%E7%AD%94%E3%80%82.mp3' type='audio/mp3'>Your browser does not support the audio tag.</audio></div><p><span class='p green'&gt;></span> 多夸夸我嘛，我会很开心的~&lt;span class='p green'><</span></p><div class="audio"><audio controls preload><source src='https://picture.adunas.top/Audio/%E7%88%B1%E8%8E%89%E5%B8%8C%E9%9B%85-%E5%97%A8%EF%BC%8C%E6%88%91%E5%8F%88%E6%9D%A5%E5%95%A6%E3%80%82%E5%A4%9A%E5%A4%B8%E5%A4%B8%E6%88%91%E5%A5%BD%E5%90%97%EF%BC%9F%E6%88%91%E4%BC%9A%E5%BE%88%E2%80%94%E2%80%94%E5%BC%80%E5%BF%83%E7%9A%84%E5%93%A6%E3%80%82.mp3' type='audio/mp3'>Your browser does not support the audio tag.</audio></div><p><span class='p green'&gt;></span> 可爱的少女心，可是无所不能的哦！&lt;span class='p green'><</span></p><div class="audio"><audio controls preload><source src='https://picture.adunas.top/Audio/%E4%BA%BA%E4%B9%8B%E5%BE%8B%E8%80%85-%E5%91%B5...%E7%9C%8B%EF%BC%8C%E5%8F%AF%E7%88%B1%E7%9A%84%E5%B0%91%E5%A5%B3%E5%BF%83%E5%8F%AF%E6%98%AF%E6%97%A0%E6%89%80%E4%B8%8D%E8%83%BD%E7%9A%84%E5%93%A6%E3%80%82.mp3' type='audio/mp3'>Your browser does not support the audio tag.</audio></div>              </div>            </details>]]></content>
    
    
    <summary type="html">🧶傻瓜，你还是跟过来了呀，我的小英雄</summary>
    
    
    
    <category term="文学" scheme="https://www.adunas.top/categories/%E6%96%87%E5%AD%A6/"/>
    
    
    <category term="小说" scheme="https://www.adunas.top/tags/%E5%B0%8F%E8%AF%B4/"/>
    
    <category term="人物档案" scheme="https://www.adunas.top/tags/%E4%BA%BA%E7%89%A9%E6%A1%A3%E6%A1%88/"/>
    
    <category term="跑团" scheme="https://www.adunas.top/tags/%E8%B7%91%E5%9B%A2/"/>
    
  </entry>
  
  <entry>
    <title>日程表：2024年02月</title>
    <link href="https://www.adunas.top/posts/20240222a.html"/>
    <id>https://www.adunas.top/posts/20240222a.html</id>
    <published>2024-02-22T08:36:32.000Z</published>
    <updated>2024-02-22T08:36:41.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文章导航总览"><a href="#文章导航总览" class="headerlink" title="文章导航总览"></a><a href="./20240221a.html#2024年2月">文章导航总览</a></h1><h1 id="2024年2月23日"><a href="#2024年2月23日" class="headerlink" title="2024年2月23日"></a>2024年2月23日</h1><div class='checkbox red checked'><input type="checkbox" checked="checked"/>            <p>运动1小时</p>            </div><ul><li>[x] 羽毛球</li></ul><div class='checkbox red'><input type="checkbox" />            <p>写一篇阅读论文的博客</p>            </div><details class="folding-tag" blue><summary> 日程表 </summary>              <div class='content'>              <div class="timeline blue"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p>时间轴</p></div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>17点46分</p></div></div><div class='timeline-item-content'><ol><li>吃晚饭</li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>18点10分</p></div></div><div class='timeline-item-content'><ol><li>阅读论文</li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>18点40分-20点40分</p></div></div><div class='timeline-item-content'><ol><li>羽毛球</li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>21点56分</p></div></div><div class='timeline-item-content'><ol><li>阅读论文</li></ol></div></div></div>              </div>            </details>]]></content>
    
    
    <summary type="html">🥐本文记录 Adunas 2024年02月的日程安排和实施情况</summary>
    
    
    
    <category term="日程表" scheme="https://www.adunas.top/categories/%E6%97%A5%E7%A8%8B%E8%A1%A8/"/>
    
    
    <category term="日程表" scheme="https://www.adunas.top/tags/%E6%97%A5%E7%A8%8B%E8%A1%A8/"/>
    
  </entry>
  
  <entry>
    <title>博客搭建导航</title>
    <link href="https://www.adunas.top/posts/20240221c.html"/>
    <id>https://www.adunas.top/posts/20240221c.html</id>
    <published>2024-02-21T11:26:31.000Z</published>
    <updated>2024-02-21T11:26:31.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文章导航总览"><a href="#文章导航总览" class="headerlink" title="文章导航总览"></a><a href="./20240221a.html#博客搭建">文章导航总览</a></h1><h1 id="博客文章语法笔记导航"><a href="#博客文章语法笔记导航" class="headerlink" title="博客文章语法笔记导航"></a>博客文章语法笔记导航</h1><h2 id="Markdown基础语法"><a href="#Markdown基础语法" class="headerlink" title="Markdown基础语法"></a><a href="./20231201a.html">Markdown基础语法</a></h2><h2 id="Markdown内置Html语法"><a href="#Markdown内置Html语法" class="headerlink" title="Markdown内置Html语法"></a><a href="./20231206b.html">Markdown内置Html语法</a></h2><h2 id="Butterfly外挂标签"><a href="#Butterfly外挂标签" class="headerlink" title="Butterfly外挂标签"></a><a href="./20231205b.html">Butterfly外挂标签</a></h2><h1 id="博客搭建教程导航"><a href="#博客搭建教程导航" class="headerlink" title="博客搭建教程导航"></a>博客搭建教程导航</h1><h2 id="基础教程"><a href="#基础教程" class="headerlink" title="基础教程"></a><a href="./20231205d.html">基础教程</a></h2><h2 id="bug汇总"><a href="#bug汇总" class="headerlink" title="bug汇总"></a><a href="./20231204c.html">bug汇总</a></h2><h2 id="未来可期"><a href="#未来可期" class="headerlink" title="未来可期"></a><a href="./20231205c.html">未来可期</a></h2><h2 id="音频教程"><a href="#音频教程" class="headerlink" title="音频教程"></a><a href="./20231207a.html">音频教程</a></h2><h2 id="文章个性化功能"><a href="#文章个性化功能" class="headerlink" title="文章个性化功能"></a><a href="./20240201a.html">文章个性化功能</a></h2><h2 id="首页美化"><a href="#首页美化" class="headerlink" title="首页美化"></a><a href="./20240202a.html">首页美化</a></h2>]]></content>
    
    
    <summary type="html">🧈本文是博客搭建的导航</summary>
    
    
    
    <category term="文章导航" scheme="https://www.adunas.top/categories/%E6%96%87%E7%AB%A0%E5%AF%BC%E8%88%AA/"/>
    
    
    <category term="文章导航" scheme="https://www.adunas.top/tags/%E6%96%87%E7%AB%A0%E5%AF%BC%E8%88%AA/"/>
    
  </entry>
  
  <entry>
    <title>编程导航</title>
    <link href="https://www.adunas.top/posts/20240221b.html"/>
    <id>https://www.adunas.top/posts/20240221b.html</id>
    <published>2024-02-21T10:48:39.000Z</published>
    <updated>2024-02-21T10:49:47.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文章导航总览"><a href="#文章导航总览" class="headerlink" title="文章导航总览"></a><a href="./20240221a.html#编程">文章导航总览</a></h1><h1 id="C"><a href="#C" class="headerlink" title="C++"></a>C++</h1><h2 id="打印"><a href="#打印" class="headerlink" title="打印"></a><a href="./20240116a.html">打印</a></h2><h1 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h1><h2 id="Git"><a href="#Git" class="headerlink" title="Git"></a><a href="./20231206a.html">Git</a></h2><h1 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h1><h2 id="前端"><a href="#前端" class="headerlink" title="前端"></a><a href="./20231211a.html">前端</a></h2><h1 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h1><h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a><a href="./20240225a.html">正则表达式</a></h2><h1 id="Latex"><a href="#Latex" class="headerlink" title="Latex"></a><a href="./20240225c.html">Latex</a></h1>]]></content>
    
    
    <summary type="html">🥞本文是编程分类的导航</summary>
    
    
    
    <category term="文章导航" scheme="https://www.adunas.top/categories/%E6%96%87%E7%AB%A0%E5%AF%BC%E8%88%AA/"/>
    
    
    <category term="文章导航" scheme="https://www.adunas.top/tags/%E6%96%87%E7%AB%A0%E5%AF%BC%E8%88%AA/"/>
    
  </entry>
  
  <entry>
    <title>文章导航</title>
    <link href="https://www.adunas.top/posts/20240221a.html"/>
    <id>https://www.adunas.top/posts/20240221a.html</id>
    <published>2024-02-21T10:41:36.000Z</published>
    <updated>2024-02-21T10:41:48.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数学"><a href="#数学" class="headerlink" title="数学"></a><a href="./20240210a.html">数学</a></h1><h1 id="英语"><a href="#英语" class="headerlink" title="英语"></a>英语</h1><h2 id="单词"><a href="#单词" class="headerlink" title="单词"></a><a href="./20231208a.html">单词</a></h2><h1 id="阅读"><a href="#阅读" class="headerlink" title="阅读"></a><a href="./20240224b.html">阅读</a></h1><h1 id="文学"><a href="#文学" class="headerlink" title="文学"></a><a href="./20240224d.html">文学</a></h1><h1 id="编程"><a href="#编程" class="headerlink" title="编程"></a><a href="./20240221b.html">编程</a></h1><h1 id="博客搭建"><a href="#博客搭建" class="headerlink" title="博客搭建"></a><a href="./20240221c.html">博客搭建</a></h1><h1 id="日程表"><a href="#日程表" class="headerlink" title="日程表"></a>日程表</h1><h2 id="2024年2月"><a href="#2024年2月" class="headerlink" title="2024年2月"></a><a href="./20240222a.html">2024年2月</a></h2><h1 id="运动健康"><a href="#运动健康" class="headerlink" title="运动健康"></a>运动健康</h1><h2 id="日常基础篇"><a href="#日常基础篇" class="headerlink" title="日常基础篇"></a><a href="./20240131a.html">日常基础篇</a></h2><h2 id="状态调整篇"><a href="#状态调整篇" class="headerlink" title="状态调整篇"></a><a href="./20240203a.html">状态调整篇</a></h2><h1 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h1><h1 id="IOS"><a href="#IOS" class="headerlink" title="IOS"></a>IOS</h1><h2 id="ipa"><a href="#ipa" class="headerlink" title="ipa"></a><a href="./20240115a.html">ipa</a></h2><h1 id="游戏"><a href="#游戏" class="headerlink" title="游戏"></a>游戏</h1><h2 id="Adunas的游戏账号昵称和ID"><a href="#Adunas的游戏账号昵称和ID" class="headerlink" title="Adunas的游戏账号昵称和ID"></a><a href="./20231201b.html">Adunas的游戏账号昵称和ID</a></h2>]]></content>
    
    
    <summary type="html">🥞本文是文章分类导航的最顶层</summary>
    
    
    
    <category term="文章导航" scheme="https://www.adunas.top/categories/%E6%96%87%E7%AB%A0%E5%AF%BC%E8%88%AA/"/>
    
    
    <category term="文章导航" scheme="https://www.adunas.top/tags/%E6%96%87%E7%AB%A0%E5%AF%BC%E8%88%AA/"/>
    
  </entry>
  
  <entry>
    <title>数学导航</title>
    <link href="https://www.adunas.top/posts/20240210a.html"/>
    <id>https://www.adunas.top/posts/20240210a.html</id>
    <published>2024-02-10T06:57:53.000Z</published>
    <updated>2024-02-21T10:43:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文章导航总览"><a href="#文章导航总览" class="headerlink" title="文章导航总览"></a><a href="./20240221a.html#数学">文章导航总览</a></h1><h1 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h1><h2 id="迹"><a href="#迹" class="headerlink" title="迹"></a><a href="./20231210a.html">迹</a></h2><h2 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a><a href="./20231204a.html">协方差</a></h2><h1 id="滤波"><a href="#滤波" class="headerlink" title="滤波"></a>滤波</h1><h2 id="卡尔曼滤波"><a href="#卡尔曼滤波" class="headerlink" title="卡尔曼滤波"></a><a href="./20231205a.html">卡尔曼滤波</a></h2><h1 id="绘图工具"><a href="#绘图工具" class="headerlink" title="绘图工具"></a>绘图工具</h1><h2 id="动态数学软件"><a href="#动态数学软件" class="headerlink" title="动态数学软件"></a><a href="./20231210b.html">动态数学软件</a></h2><p>&emsp;&emsp;动态数学软件GroGebra。</p>]]></content>
    
    
    <summary type="html">🥧本文是数学分类的导航</summary>
    
    
    
    <category term="文章导航" scheme="https://www.adunas.top/categories/%E6%96%87%E7%AB%A0%E5%AF%BC%E8%88%AA/"/>
    
    
    <category term="文章导航" scheme="https://www.adunas.top/tags/%E6%96%87%E7%AB%A0%E5%AF%BC%E8%88%AA/"/>
    
  </entry>
  
  <entry>
    <title>运动健康（二）：状态调整篇</title>
    <link href="https://www.adunas.top/posts/20240203a.html"/>
    <id>https://www.adunas.top/posts/20240203a.html</id>
    <published>2024-02-03T04:32:11.000Z</published>
    <updated>2024-02-03T04:32:11.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文章导航总览"><a href="#文章导航总览" class="headerlink" title="文章导航总览"></a><a href="./20240221a.html#状态调整篇">文章导航总览</a></h1><blockquote><h1 id="运动健康的导航"><a href="#运动健康的导航" class="headerlink" title="运动健康的导航"></a>运动健康的导航</h1><ol><li><a href="./20240131a.html">运动健康（一）：日常基础篇</a></li><li><a href="./20240203a.html">运动健康（二）：状态调整篇</a>⇦当前位置🪂</li></ol></blockquote><div class="note info flat"></div><h1 id="独立自主"><a href="#独立自主" class="headerlink" title="独立自主"></a>独立自主</h1><p>&emsp;&emsp;学会自尊自爱，任何时候不能把自己的未来托付给别人。你最亲的父母不可以，最好的朋友不可以，最爱的女朋友不可以，最知心的老师也不可以。你唯一要值得托付的人只有自己，也只能是自己。自己是自己的父母，照顾自己衣食起居。自己是自己的孩子，纯真、理想、真心都在此。</p><h1 id="压力别人"><a href="#压力别人" class="headerlink" title="压力别人"></a>压力别人</h1><p>&emsp;&emsp;偶尔还是有这个坏习惯。压力别人典型是自卑的表现，想通过打压别人来体现自身的价值，这只是嫉妒，是很不健康的，特别是在两性关系上。生活已经很累啦，她要的是情绪价值，而不是又多一个压力的老师。多站在别人的角度去想想、去关心、去爱吧。学会<a href="#赞美">赞美</a>别人。</p><h1 id="沉默与激情"><a href="#沉默与激情" class="headerlink" title="沉默与激情"></a>沉默与激情</h1><p>&emsp;&emsp;不求无功，但求无过。这句话说得真差！我来改改：不求无功，不怕犯错！这段时间免去了很多无用的社交、无用的焦虑。沉心静气地搞学习和研究，并不觉得孤独，反而觉得无比踏实、舒心。合适的社交，我会打破沉默，我要从激情地学习转变成热情地交流，真诚面对每一个人，和不同的人、合适的人交流才能让自己能够不让自己的思想和视野变得狭隘和偏颇。热情会被打击，但是那简直是挠痒痒，因为我们根本<a href="#不生气不伤心">不生气不伤心</a>呀。</p><h1 id="不生气不伤心"><a href="#不生气不伤心" class="headerlink" title="不生气不伤心"></a>不生气不伤心</h1><p>&emsp;&emsp;你要始终明白自己在乎什么，什么是对你重要的。不论男女，最重要的是做好自己该做的事情。对于我，男生来说，最重要的是事业，事业做好了，她有可能跟你，事业做不好，她一定不会跟你。所以你想想，真的是别人让你心情不好了嘛？答案是否定的，而是自己把自己心情变得不好了。</p><p>&emsp;&emsp;我跟老师聊过。我跟同学聊过。我跟家人聊过。我跟朋友聊过。我也跟自己聊过。如果我能跟她再聊一聊就更好了。开心地、努力地、自信地做手头上的事情，就是最棒了。有好身体、好工作、好心态就已经完胜啦。</p><h1 id="赞美"><a href="#赞美" class="headerlink" title="赞美"></a>赞美</h1><p>&emsp;&emsp;以前我很讨厌阿谀奉承，然而这种讨厌被无形地扩大了。扩大到不会由衷地欣赏赞美别人。而且阿谀奉承我现在根本不讨厌了，但我不会去阿谀奉承。如果我需要帮助，我会真诚地表达、寻求帮助。我的赞美也不会是阿谀奉承，而是要通过观察后，真的能发现这件事带给我们的美，以及那背后的故事~</p><p>&emsp;&emsp;为什么我不讨厌阿谀奉承。因为我知道人的生活是艰难的，他只不过在艰难地在夹缝中生存着，他阿谀奉承几句，并不是出于恶意，而是可以保住自己的工作，保住自己的饭碗，有个更好的机会而已。说几句话能让大家都开心，这有什么不对吗？这样在工作中大家都很舒服，这是极其正确的。</p><p>&emsp;&emsp;推荐用更好的 “阿谀奉承” 的方法，那便是赞美。学会认可别人的工作，学会欣赏，学会赞美。比如她画了一个妆，男生可能看了并没有什么和之前感觉不一样，但是你不知道的是，她为了你化妆准备了多久。她平时一个人的时候可是懒得打扮呀，这时候，我觉得这个女生是真的很美，很可爱呀。我会仔细地看看她地眼睛、腮红，虽然我对化妆一窍不通，但是好像真的有些不一样，我一定会开心得看着她说：你今天真美！</p><p>所以你在和别人交流的时候，总有些东西你不太懂，但是对方极力讲的时候，一定是对他非常重要的东西吧！他一定为这件事付出了很多实实在在的努力，这个时候我会大方地赞美，因为我确实能被感染到，这个时候，我不认为是什么阿谀奉承。</p><blockquote><h1 id="运动健康的导航-1"><a href="#运动健康的导航-1" class="headerlink" title="运动健康的导航"></a>运动健康的导航</h1><ol><li><a href="./20240131a.html">运动健康（一）：日常基础篇</a></li><li><a href="./20240203a.html">运动健康（二）：状态调整篇</a>⇦当前位置🪂</li></ol></blockquote>]]></content>
    
    
    <summary type="html">🍫本文总结状态调整的方法</summary>
    
    
    
    <category term="运动健康" scheme="https://www.adunas.top/categories/%E8%BF%90%E5%8A%A8%E5%81%A5%E5%BA%B7/"/>
    
    
    <category term="健康" scheme="https://www.adunas.top/tags/%E5%81%A5%E5%BA%B7/"/>
    
    <category term="心态" scheme="https://www.adunas.top/tags/%E5%BF%83%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>博客搭建教程：首页美化</title>
    <link href="https://www.adunas.top/posts/20240202a.html"/>
    <id>https://www.adunas.top/posts/20240202a.html</id>
    <published>2024-02-02T15:36:47.000Z</published>
    <updated>2024-02-21T12:29:30.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="博客搭建教程导航"><a href="#博客搭建教程导航" class="headerlink" title="博客搭建教程导航"></a><a href="./20240221c.html#首页美化">博客搭建教程导航</a></h1><h1 id="格言"><a href="#格言" class="headerlink" title="格言"></a>格言</h1><p>&emsp;&emsp;在[页脚配置文件]./themes/butterfly/layout/includes/footer.pug 中，修改如下代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.t-t-l</span><br><span class="line">          p.ft-t.t-l-t 格言🧬</span><br><span class="line">          .bg-ad</span><br><span class="line">            div</span><br><span class="line">              | 再看看那个光点，它就在这里，这是家园，这是我们 —— 你所爱的每一个人，你认识的一个人，你听说过的每一个人，曾经有过的每一个人，都在它上面度过他们的一生✨</span><br><span class="line">            .btn-xz-box</span><br><span class="line">              a.btn-xz(href=&#x27;https://stellarium.org/&#x27;) 点击开启星辰之旅</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">🍟本文记录博客首页美化的方法</summary>
    
    
    
    <category term="博客" scheme="https://www.adunas.top/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="hexo" scheme="https://www.adunas.top/tags/hexo/"/>
    
    <category term="html" scheme="https://www.adunas.top/tags/html/"/>
    
  </entry>
  
  <entry>
    <title>博客搭建教程：文章个性化功能</title>
    <link href="https://www.adunas.top/posts/20240201a.html"/>
    <id>https://www.adunas.top/posts/20240201a.html</id>
    <published>2024-02-01T03:30:06.000Z</published>
    <updated>2024-02-21T12:29:30.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="博客搭建教程导航"><a href="#博客搭建教程导航" class="headerlink" title="博客搭建教程导航"></a><a href="./20240221c.html#文章个性化功能">博客搭建教程导航</a></h1><h1 id="文章页Html标签"><a href="#文章页Html标签" class="headerlink" title="文章页Html标签"></a>文章页Html标签</h1><p>&emsp;&emsp;有时候我们想在博客文章里添加一些 hexo 不具有的特性时，就可以在 markdown 文件中添加 Html 标签。</p><p>&emsp;&emsp;html、CSS 和 js 可以分开写，也分三个文件写，html始终放在 markdown文件里。分开写的话，CSS 和 js 文件要放在主题的源文件路径 /themes/butterfly/source/ 下的的 css 或者 js 文件夹下。在 Markdown 里的语法为：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text/css&quot;</span> <span class="attr">href</span>=<span class="string">&quot;/css/grid.css&quot;</span>&gt;</span></span></span><br><span class="line"><span class="code">    </span></span><br><span class="line"><span class="code">&lt;div class=&quot;container1&quot;&gt;  </span></span><br><span class="line"><span class="code">    &lt;iframe class=&quot;container-iframe&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;  </span></span><br><span class="line"><span class="code">&lt;/div&gt;</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span> <span class="attr">src</span>=<span class="string">&quot;/js/grid.js&quot;</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h1 id="Bilibili视频适配"><a href="#Bilibili视频适配" class="headerlink" title="Bilibili视频适配"></a>Bilibili视频适配</h1><div class="note purple no-icon flat"><p>参考文章：<a href="https://www.fomal.cc/posts/5389e93f.html">Bilibili视频适配</a></p></div><ol><li>在[BlogRoot]\source\css\custom.css自定义样式的文件中引入如下代码（这是我的，你可以自行微调）：</li></ol><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*哔哩哔哩视频适配*/</span></span><br><span class="line"><span class="selector-class">.aspect-ratio</span> &#123;</span><br><span class="line">  <span class="attribute">position</span>: relative;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">90%</span>;</span><br><span class="line">  <span class="attribute">height</span>: auto;</span><br><span class="line">  <span class="attribute">padding-bottom</span>: <span class="number">75%</span>;</span><br><span class="line">  <span class="attribute">margin</span>: <span class="number">3%</span> auto;</span><br><span class="line">  <span class="attribute">text-align</span>: center;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.aspect-ratio</span> <span class="selector-tag">iframe</span> &#123;</span><br><span class="line">  <span class="attribute">position</span>: absolute;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">  <span class="attribute">height</span>: <span class="number">86%</span>;</span><br><span class="line">  <span class="attribute">left</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">top</span>: <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>直接复制插入你的 md 文章就行，修改里面的 src 源为你的视频：</li></ol><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">align</span>=<span class="string">center</span> <span class="attr">class</span>=<span class="string">&quot;aspect-ratio&quot;</span>&gt;</span></span></span><br><span class="line"><span class="code">    &lt;iframe src=&quot;https://player.bilibili.com/player.html?aid=474023258&amp;&amp;page=1&amp;as_wide=1&amp;high_quality=1&amp;danmaku=0&amp;autoplay=0 &quot; </span></span><br><span class="line"><span class="code">    scrolling=&quot;no&quot; </span></span><br><span class="line"><span class="code">    border=&quot;0&quot; </span></span><br><span class="line"><span class="code">    frameborder=&quot;no&quot; </span></span><br><span class="line"><span class="code">    framespacing=&quot;0&quot; </span></span><br><span class="line"><span class="code">    high_quality=1</span></span><br><span class="line"><span class="code">    danmaku=1 </span></span><br><span class="line"><span class="code">    allowfullscreen=&quot;true&quot;&gt; </span></span><br><span class="line"><span class="code">    &lt;/iframe&gt;</span></span><br><span class="line"><span class="code">&lt;/div&gt;</span></span><br></pre></td></tr></table></figure><p>源视频的链接获取方法为：在b站官网分享视频时，选择 <code>嵌入代码</code>。若要关闭视频自动播放，在后面添加参数：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&amp;autoplay=0</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">🍔本文记录博客文章内插入的个性化功能</summary>
    
    
    
    <category term="博客" scheme="https://www.adunas.top/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="hexo" scheme="https://www.adunas.top/tags/hexo/"/>
    
    <category term="html" scheme="https://www.adunas.top/tags/html/"/>
    
  </entry>
  
  <entry>
    <title>运动健康：日常基础篇</title>
    <link href="https://www.adunas.top/posts/20240131a.html"/>
    <id>https://www.adunas.top/posts/20240131a.html</id>
    <published>2024-01-31T14:47:09.000Z</published>
    <updated>2024-02-02T11:54:47.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文章导航总览"><a href="#文章导航总览" class="headerlink" title="文章导航总览"></a><a href="./20240221a.html#日常基础篇">文章导航总览</a></h1><blockquote><h1 id="运动健康的导航"><a href="#运动健康的导航" class="headerlink" title="运动健康的导航"></a>运动健康的导航</h1><ol><li><a href="./20240131a.html">运动健康（一）：日常基础篇</a>⇦当前位置🪂</li><li><a href="./20240203a.html">运动健康（二）：状态调整篇</a></li></ol></blockquote><h1 id="颈椎"><a href="#颈椎" class="headerlink" title="颈椎"></a>颈椎</h1><p>&emsp;&emsp;<a href="https://www.bilibili.com/video/BV1Yb411b7nd/?share_source=copy_web&amp;vd_source=6b55cb6788b1952e04c06b095d772810">【有来医生 8招颈椎操全教程】</a></p><div align=center class="aspect-ratio">    <iframe src="https://player.bilibili.com/player.html?aid=45039749&bvid=BV1Yb411b7nd&cid=78880311&p=1&autoplay=0 "     scrolling="no"     border="0"     frameborder="no"     framespacing="0"     high_quality=1    danmaku=1     allowfullscreen="true">     </iframe></div><h1 id="腰椎"><a href="#腰椎" class="headerlink" title="腰椎"></a>腰椎</h1><p>&emsp;&emsp;<a href="https://www.bilibili.com/video/BV1fp4y1U7qG/?share_source=copy_web&amp;vd_source=6b55cb6788b1952e04c06b095d772810">【每天5分钟 <em>告别骨盆前倾，小肚子突出，大屁股</em>】</a></p><div align=center class="aspect-ratio">    <iframe src="https://player.bilibili.com/player.html?aid=968747403&bvid=BV1fp4y1U7qG&cid=205794890&p=1&autoplay=0 "     scrolling="no"     border="0"     frameborder="no"     framespacing="0"     high_quality=1    danmaku=1     allowfullscreen="true">     </iframe></div><blockquote><h1 id="运动健康的导航-1"><a href="#运动健康的导航-1" class="headerlink" title="运动健康的导航"></a>运动健康的导航</h1><ol><li><a href="./20240131a.html">运动健康（一）：日常基础篇</a>⇦当前位置🪂</li><li><a href="./20240203a.html">运动健康（二）：状态调整篇</a></li></ol></blockquote>]]></content>
    
    
    <summary type="html">🎈本文汇总日常热身的内容</summary>
    
    
    
    <category term="运动健康" scheme="https://www.adunas.top/categories/%E8%BF%90%E5%8A%A8%E5%81%A5%E5%BA%B7/"/>
    
    
    <category term="运动" scheme="https://www.adunas.top/tags/%E8%BF%90%E5%8A%A8/"/>
    
    <category term="健身" scheme="https://www.adunas.top/tags/%E5%81%A5%E8%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>C++ 打印</title>
    <link href="https://www.adunas.top/posts/20240116a.html"/>
    <id>https://www.adunas.top/posts/20240116a.html</id>
    <published>2024-01-16T13:41:33.000Z</published>
    <updated>2024-02-21T11:20:22.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="编程导航"><a href="#编程导航" class="headerlink" title="编程导航"></a><a href="./20240221b.html#打印">编程导航</a></h1><h1 id="C"><a href="#C" class="headerlink" title="C++"></a>C++</h1><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><h4 id="方法1"><a href="#方法1" class="headerlink" title="方法1"></a>方法1</h4><p>&emsp;&emsp;调用库：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br></pre></td></tr></table></figure><p>调用函数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std::cout &lt;&lt; <span class="string">&quot;Hello!\n&quot;</span>;</span><br></pre></td></tr></table></figure><h4 id="方法2"><a href="#方法2" class="headerlink" title="方法2"></a>方法2</h4><p>&emsp;&emsp;命名空间，省略 std。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br></pre></td></tr></table></figure><p>调用函数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cout &lt;&lt; <span class="string">&quot;Hello!\n&quot;</span>;</span><br></pre></td></tr></table></figure><h3 id="组合方式"><a href="#组合方式" class="headerlink" title="组合方式"></a>组合方式</h3><h4 id="组合方式1"><a href="#组合方式1" class="headerlink" title="组合方式1"></a>组合方式1</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std::cout &lt;&lt; <span class="string">&quot;Hello World!\n&quot;</span>;</span><br></pre></td></tr></table></figure><h4 id="组合方式2"><a href="#组合方式2" class="headerlink" title="组合方式2"></a>组合方式2</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std::cout &lt;&lt; <span class="string">&quot;Hello&quot;</span> &lt;&lt; <span class="string">&quot; World!\n&quot;</span>;</span><br></pre></td></tr></table></figure><h4 id="组合方式3"><a href="#组合方式3" class="headerlink" title="组合方式3"></a>组合方式3</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">std::cout &lt;&lt; <span class="string">&quot;Hello&quot;</span>;</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot; World!\n&quot;</span>;</span><br></pre></td></tr></table></figure><h3 id="特殊符号"><a href="#特殊符号" class="headerlink" title="特殊符号"></a>特殊符号</h3><h4 id="回车"><a href="#回车" class="headerlink" title="回车"></a>回车</h4><p>&emsp;&emsp;回车表示将光标移动到行的开头。这应该和键盘上的回车有所区分。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std::cout &lt;&lt; <span class="string">&quot;\r&quot;</span>;</span><br></pre></td></tr></table></figure><h4 id="换行"><a href="#换行" class="headerlink" title="换行"></a>换行</h4><p>&emsp;&emsp;换行表示将光标移动到下一行的开头。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std::cout &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br></pre></td></tr></table></figure><p>或者：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std::cout &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure><h3 id="变量调用"><a href="#变量调用" class="headerlink" title="变量调用"></a>变量调用</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> num = <span class="number">100</span>;</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;数字是：&quot;</span> &lt;&lt; num &lt;&lt; std::endl;</span><br><span class="line">std::string st = <span class="string">&quot;你好，世界！&quot;</span>;</span><br><span class="line">std::cout &lt;&lt; st &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure><h3 id="演示"><a href="#演示" class="headerlink" title="演示"></a>演示</h3><p>&emsp;&emsp;演示效果如下：</p><div class="tabs" id="print"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#print-1">示例源码1</button></li><li class="tab"><button type="button" data-href="#print-2">演示结果1</button></li><li class="tab"><button type="button" data-href="#print-3">示例源码2</button></li><li class="tab"><button type="button" data-href="#print-4">演示结果2</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="print-1"><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Print\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;|_&quot;</span> &lt;&lt; <span class="string">&quot;语法\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| |_&quot;</span> &lt;&lt; <span class="string">&quot;基础\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| | |_&quot;</span> &lt;&lt; <span class="string">&quot;Hello!\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| |&quot;</span> &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| |_&quot;</span> &lt;&lt; <span class="string">&quot;组合方式\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| | |_&quot;</span> &lt;&lt; <span class="string">&quot;组合方式1\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| | | |_&quot;</span> &lt;&lt; <span class="string">&quot;Hello World!\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| | |&quot;</span> &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| | |_&quot;</span> &lt;&lt; <span class="string">&quot;组合方式2\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| | | |_&quot;</span> &lt;&lt; <span class="string">&quot;Hello&quot;</span> &lt;&lt; <span class="string">&quot; World!\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| | |&quot;</span> &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| | |_&quot;</span> &lt;&lt; <span class="string">&quot;组合方式3\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| |   |_&quot;</span> &lt;&lt; <span class="string">&quot;Hello&quot;</span>; std::cout &lt;&lt; <span class="string">&quot; World!\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| |&quot;</span> &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| |_&quot;</span> &lt;&lt; <span class="string">&quot;特殊符号\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| | |_&quot;</span> &lt;&lt; <span class="string">&quot;回车\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| |   |_&quot;</span> &lt;&lt; <span class="string">&quot;\r&quot;</span>; std::cout &lt;&lt; <span class="string">&quot;*\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| | |_&quot;</span> &lt;&lt; <span class="string">&quot;换行\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| |   |_&quot;</span> &lt;&lt; <span class="string">&quot;\n&quot;</span>; std::cout &lt;&lt; <span class="string">&quot;*\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| |   |_&quot;</span> &lt;&lt; <span class="string">&quot;或者\n&quot;</span>; </span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| |   |_&quot;</span> &lt;&lt; std::endl; std::cout &lt;&lt; <span class="string">&quot;*\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| |&quot;</span> &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| |_&quot;</span> &lt;&lt; <span class="string">&quot;变量调用\n&quot;</span>;</span><br><span class="line">    <span class="type">int</span> num = <span class="number">100</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| | |_&quot;</span> &lt;&lt; <span class="string">&quot;数字是：&quot;</span> &lt;&lt; num &lt;&lt; std::endl;</span><br><span class="line">    std::string st = <span class="string">&quot;你好，世界！&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;| | |_&quot;</span> &lt;&lt; st &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="print-2"><p><img src="https://picture.adunas.top/Program/PrintCppVs2022A.png" alt=""></p><p>说明：换行和回车有明显区别。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="print-3"><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 有三处可以省略 std::</span></span><br><span class="line">    string st = <span class="string">&quot;你好，世界！&quot;</span>;</span><br><span class="line">    cout &lt;&lt; st &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="print-4"><p><img src="https://picture.adunas.top/Program/PrintCppVs2022B.png" alt=""></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>]]></content>
    
    
    <summary type="html">🍖本文汇总 C++ 打印显示的功能</summary>
    
    
    
    <category term="编程" scheme="https://www.adunas.top/categories/%E7%BC%96%E7%A8%8B/"/>
    
    
    <category term="C++" scheme="https://www.adunas.top/tags/C/"/>
    
    <category term="print" scheme="https://www.adunas.top/tags/print/"/>
    
  </entry>
  
</feed>
